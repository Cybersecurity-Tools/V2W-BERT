{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# // ***********************************************************************\n",
    "# //\n",
    "# //        V2W-BERT: A Python library for vulnerability classification\n",
    "# //               Siddhartha Das (das90@purdue.edu) : Purdue University\n",
    "# //               Mahantesh Halappanavar (hala@pnnl.gov): Pacific Northwest National Laboratory   \n",
    "# //\n",
    "# // ***********************************************************************\n",
    "\n",
    " \n",
    "# Copyright Â© 2022, Battelle Memorial Institute\n",
    "# All rights reserved.\n",
    "\n",
    " \n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    " \n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "\n",
    "#    list of conditions and the following disclaimer.\n",
    "\n",
    " \n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "\n",
    "#    this list of conditions and the following disclaimer in the documentation\n",
    "\n",
    "#    and/or other materials provided with the distribution.\n",
    "\n",
    " \n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  24\n"
     ]
    }
   ],
   "source": [
    "import torch# If there's a GPU available...\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "# device = torch.device(\"cpu\")  \n",
    "# print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading directory:  ./Results\n",
      "Model Saving directory: ./Results/Model/\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Dataset import getDataset, getDummyDataset, Data        \n",
    "\n",
    "DIR='./Results'\n",
    "    \n",
    "from pathlib import Path\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_LOAD_DIR=\"./Dataset/NVD/processed/\"\n",
    "MODEL_SAVE_DIR=DIR+'/Model/'\n",
    "\n",
    "Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Data loading directory: \", DIR)\n",
    "print(\"Model Saving directory:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import zipfile\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "from random import sample\n",
    "\n",
    "seed_val = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "try:\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except:\n",
    "    print(\"nothing to set for cudnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible choices for pretrained are:\n",
    "distilbert-base-uncased\n",
    "bert-base-uncased\n",
    "\n",
    "The BERT paper says: \"[The] pre-trained BERT model can be fine-tuned with just one additional output\n",
    "layer to create state-of-the-art models for a wide range of tasks, such as question answering and\n",
    "language inference, without substantial task-specific architecture modifications.\"\n",
    "\n",
    "Huggingface/transformers provides access to such pretrained model versions, some of which have been\n",
    "published by various community members.\n",
    "\n",
    "BertForSequenceClassification is one of those pretrained models, which is loaded automatically by\n",
    "AutoModelForSequenceClassification because it corresponds to the pretrained weights of\n",
    "\"bert-base-uncased\".\n",
    "\n",
    "Huggingface says about BertForSequenceClassification: Bert Model transformer with a sequence\n",
    "classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE\n",
    "tasks.\"\n",
    "\n",
    "This part is easy  we instantiate the pretrained model (checkpoint)\n",
    "\n",
    "But it's also incredibly important, e.g. by using \"bert-base-uncased, we determine, that that model\n",
    "does not distinguish between lower and upper case. This might have a significant impact on model\n",
    "performance!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        # a very useful feature of pytorch lightning  which leads to the named variables that are passed in\n",
    "        # being available as self.hparams.<variable_name> We use this when refering to eg\n",
    "        # self.hparams.learning_rate\n",
    "\n",
    "        # freeze\n",
    "        self._frozen = False\n",
    "\n",
    "        # eg https://github.com/stefan-it/turkish-bert/issues/5\n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "\n",
    "        #print(config)\n",
    "\n",
    "        A = AutoModelForMaskedLM\n",
    "        self.model = A.from_pretrained(self.hparams.pretrained, config=config)\n",
    "\n",
    "        print('Model: ', type(self.model))\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        # there are some choices, as to how you can define the input to the forward function I prefer it this\n",
    "        # way, where the batch contains the input_ids, the input_put_mask and sometimes the labels (for\n",
    "        # training)\n",
    "        \n",
    "        #print(batch['input_ids'].shape)\n",
    "        #print(batch['labels'].shape)\n",
    "                \n",
    "        outputs = self.model(input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'])\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "    \n",
    "#     def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n",
    "#         self.t2=time.time()\n",
    "    \n",
    "#     def on_train_epoch_start(self):\n",
    "#         self.t0=time.time()\n",
    "    \n",
    "    \n",
    "#     def on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \n",
    "#         if batch_idx%50 ==0:\n",
    "#             t1=time.time()        \n",
    "#             #print(\"Batch {0} of {1}: {2:.6f}\".format(batch_idx+1, self.total_train_batches, t1-self.t0))\n",
    "#             print(\"Batch {0}: {1:.4f}\".format(batch_idx, t1-self.t0))\n",
    "    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class stipulates you to overwrite. This we do here, by virtue of this definition\n",
    "        \n",
    "        # self refers to the model, which in turn acceses the forward method\n",
    "        loss = self(batch)\n",
    "        \n",
    "        #tensorboard_logs = {'train_loss': loss}\n",
    "        # pytorch lightning allows you to use various logging facilities, eg tensorboard with tensorboard we\n",
    "        # can track and easily visualise the progress of training. In this case\n",
    "        \n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        #self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        #return {'loss': loss, 'log': tensorboard_logs}\n",
    "        # the training_step method expects a dictionary, which should at least contain the loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss = self(batch)\n",
    "        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return val_loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss = self(batch)\n",
    "        self.log('test_loss', test_loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return test_loss\n",
    "    \n",
    "#     def on_train_epoch_end(self, train_step_outputs):\n",
    "#         print('Epoch ',self.current_epoch,' done: took ',time.time()-self.t0, ' sec')\n",
    "\n",
    "        #---------------------        \n",
    "#         print(train_step_outputs)        \n",
    "#         import pdb; pdb.set_trace()\n",
    "\n",
    "#         avg_loss = torch.stack([x['loss'] for x in train_step_outputs]).mean()\n",
    "    \n",
    "#         print(torch.stack([x['loss'] for x in train_step_outputs]))\n",
    "#         tensorboard_logs = {'train_loss': avg_loss}\n",
    "        \n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "#         self.log('log', tensorboard_logs, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "#         return {\n",
    "#             'train_loss': avg_loss,\n",
    "#             'log': tensorboard_logs,\n",
    "#             'progress_bar': {\n",
    "#                 'train_loss': avg_loss\n",
    "#             }\n",
    "#         }\n",
    "#---------------------\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # The configure_optimizers is a (virtual) method, specified in the interface, that the\n",
    "        # pl.LightningModule class wants you to overwrite.\n",
    "        # In this case we define that some parameters are optimized in a different way than others. In\n",
    "        # particular we single out parameters that have 'bias', 'LayerNorm.weight' in their names. For those\n",
    "        # we do not use an optimization technique called weight decay.\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.01\n",
    "        }, {\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.0\n",
    "        }]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0, # Default value in run_glue.py\n",
    "            num_training_steps=self.hparams.num_training_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "#---------------------\n",
    "#     def freeze(self) -> None:\n",
    "#         # freeze all layers, except the final classifier layers\n",
    "#         for name, param in self.model.named_parameters():\n",
    "#             if 'classifier' not in name:  # classifier layer\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "\n",
    "#         self._frozen = True\n",
    "\n",
    "#     def unfreeze(self) -> None:\n",
    "#         if self._frozen:\n",
    "#             for name, param in self.model.named_parameters():\n",
    "#                 if 'classifier' not in name:  # classifier layer\n",
    "#                     param.requires_grad = True\n",
    "\n",
    "#         self._frozen = False\n",
    "\n",
    "#     def on_epoch_start(self):\n",
    "#         \"\"\"pytorch lightning hook\"\"\"\n",
    "#         if self.current_epoch < self.hparams.nr_frozen_epochs:\n",
    "#             self.freeze()\n",
    "\n",
    "#         if self.current_epoch >= self.hparams.nr_frozen_epochs:\n",
    "#             self.unfreeze()\n",
    "#---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we finally arrive at the definition of our data class derived from pl.LightningDataModule.\n",
    "\n",
    "In earlier versions of pytorch lightning  (prior to 0.9) the methods here were part of the model class\n",
    "derived from pl.LightningModule. For better flexibility and readability the Data and Model related parts\n",
    "were split out into two different classes:\n",
    "\n",
    "pl.LightningDataModule and pl.LightningModule\n",
    "\n",
    "with the Model related part remaining in pl.LightningModule\n",
    "\n",
    "This is explained in more detail in this video: https://www.youtube.com/watch?v=L---MBeSXFw\n",
    "```\n",
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "```\n",
    "\n",
    "\n",
    "Another testing code\n",
    "\n",
    "```\n",
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, collator, labels):\n",
    "        self.encodings = encodings\n",
    "        self.collator = collator\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}        \n",
    "        item = self.collator([item])\n",
    "        item = {key: val[0] for key, val in item.items()}\n",
    "    \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "\n",
    "train_encodings = berttokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "dataset = CDataset(train_encodings,datacollator)\n",
    "\n",
    "dataset[0]\n",
    "```\n",
    "\n",
    "cf this open issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/3232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, collator):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.collator = collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        item = self.collator([item])\n",
    "        item = {key: val[0] for key, val in item.items()}\n",
    "    \n",
    "        if type(self.labels)!=torch.Tensor:\n",
    "            item['org_labels']= torch.tensor(self.labels[idx])\n",
    "        else:\n",
    "            item['org_labels']= self.labels[idx]\n",
    "    \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class DataProcessing(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.save_hyperparameters()\n",
    "        if isinstance(args, tuple): args = args[0]\n",
    "        self.hparams = args\n",
    "        self.batch_size=self.hparams.batch_size        \n",
    "\n",
    "#         print('args:', args)\n",
    "#         print('kwargs:', kwargs)\n",
    "#         print(f'self.hparams.pretrained:{self.hparams.pretrained}')\n",
    "\n",
    "        #print('Loading BERT tokenizer')\n",
    "        print(f'PRETRAINED:{self.hparams.pretrained}')\n",
    "\n",
    "        A = AutoTokenizer\n",
    "        self.tokenizer = A.from_pretrained(self.hparams.pretrained, use_fast=True)\n",
    "\n",
    "        print('Tokenizer:', type(self.tokenizer))\n",
    "        \n",
    "        self.datacollator=DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=0.15)\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        MAX_TEXT_LENGTH=512\n",
    "    \n",
    "#         CVE dataset\n",
    "#         ------------------------------------        \n",
    "        data, df_CVE, df_CWE=None,None,None\n",
    "    \n",
    "        if self.hparams.rand_dataset=='dummy':            \n",
    "            #------------------------------------\n",
    "            data, sentences, labels = getDummyDataset()        \n",
    "            #------------------------------------\n",
    "        else:        \n",
    "            if self.hparams.rand_dataset=='temporal':\n",
    "                print(\"Temporal Partition:--\")\n",
    "                data, df_CVE, df_CWE = getDataset(DATASET_LOAD_DIR)            \n",
    "            else:\n",
    "                print(\"Random Partition:--\")\n",
    "                data, df_CVE, df_CWE = getRandomDataset(DATASET_LOAD_DIR, 0.70, 0.10)\n",
    "                \n",
    "            #print(df_CVE)\n",
    "\n",
    "            sentences=df_CVE['CVE Description'].apply(lambda x: str(x)[:MAX_TEXT_LENGTH])\n",
    "            \n",
    "            #print(sentences)\n",
    "            \n",
    "            labels=data.y\n",
    "            CWE_IDS_USED=df_CWE['Name'].tolist()\n",
    "            INDEX_TO_CWE_MAP=dict(zip(list(range(len(CWE_IDS_USED))),CWE_IDS_USED))\n",
    "            CWE_TO_INDEX_MAP=dict(zip(CWE_IDS_USED,list(range(len(CWE_IDS_USED)))))\n",
    "            sentences=sentences.tolist()\n",
    "        \n",
    "        \n",
    "        if type(labels)!=torch.Tensor:\n",
    "            labels=torch.tensor(labels,dtype=torch.long)\n",
    "        else:\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        self.NUM_CLASSES=len(data.y[0])\n",
    "    \n",
    "        train_encodings = self.tokenizer(sentences, truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        self.dataset = CDataset(train_encodings, labels, self.datacollator)        \n",
    "        \n",
    "        val_mask= (data.val_mask == True).nonzero().flatten().numpy()\n",
    "        val_encodings = self.tokenizer([sentences[i] for i in val_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.val_dataset=CDataset(val_encodings, labels[data.val_mask], self.datacollator)\n",
    "        \n",
    "        test_mask= (data.test_mask == True).nonzero().flatten().numpy()\n",
    "        test_encodings = self.tokenizer([sentences[i] for i in test_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.test_dataset=CDataset(test_encodings, labels[data.test_mask], self.datacollator)\n",
    "                \n",
    "        #print('Example Sentence[0]: ', sentences[0])              \n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train_sampler = RandomSampler(self.dataset)\n",
    "        \n",
    "        return DataLoader(self.dataset,\n",
    "                         #sampler=train_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size)\n",
    "                         )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val_sampler = SequentialSampler(self.val_dataset)\n",
    "        \n",
    "        return DataLoader(self.val_dataset,\n",
    "                          sampler=val_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size)\n",
    "                         )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \n",
    "        test_sampler = SequentialSampler(self.test_dataset)\n",
    "        \n",
    "        return DataLoader(self.test_dataset,\n",
    "                          sampler=test_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelParams(model):\n",
    "    print (model)\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "    print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-5:]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_value(model):\n",
    "    params = list(model.named_parameters())\n",
    "    print (params[-1][0],params[-1][1][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Two key aspects:\n",
    "\n",
    "- pytorch lightning can add arguments to the parser automatically\n",
    "- you can manually add your own specific arguments.\n",
    "\n",
    "- there is a little more code than seems necessary, because of a particular argument the scheduler\n",
    "  needs. There is currently an open issue on this complication\n",
    "  https://github.com/PyTorchLightning/pytorch-lightning/issues/1038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Batching (not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if args.auto_batch>0:    \n",
    "        #init_batch_size=32\n",
    "        init_batch_size=args.auto_batch\n",
    "        tuner = Tuner(trainer)        \n",
    "        assert hasattr(dataProcessor, \"batch_size\")\n",
    "        new_batch_size = tuner.scale_batch_size(model, \n",
    "                                                mode=\"binsearch\", \n",
    "                                                init_val=init_batch_size, \n",
    "                                                max_trials=10,\n",
    "                                                datamodule=dataProcessor,                                        \n",
    "                                               )\n",
    "\n",
    "        print(\"Max batch size: \", new_batch_size)\n",
    "        #dataProcessor.batch_size = new_batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "    parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\") \n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--nr_frozen_epochs', type=int, default=5)\n",
    "    parser.add_argument('--training_portion', type=float, default=0.9)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--frac', type=float, default=1)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--nodes', type=int, default=1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--refresh_rate', type=int, default=1)\n",
    "    parser.add_argument('--check', type=bool, default=False)\n",
    "    parser.add_argument('--rand_dataset', type=str, default=\"temporal\", choices=['temporal','random','dummy'])\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    # parser = Model.add_model_specific_args(parser) parser = Data.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"BATCH SIZE: \", args.batch_size)\n",
    "    \n",
    "    # start : get training steps\n",
    "    dataProcessor = DataProcessing(args)\n",
    "    dataProcessor.setup()\n",
    "    \n",
    "    args.num_training_steps = len(dataProcessor.train_dataloader())*args.epochs\n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    gpus=-1\n",
    "    if NUM_GPUS>0:\n",
    "        gpus=args.num_gpus        \n",
    "    else:\n",
    "        args.parallel_mode=None\n",
    "        gpus=None\n",
    "    \n",
    "    print(\"USING GPUS:\", gpus)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='loss_epoch',\n",
    "        dirpath=MODEL_SAVE_DIR,\n",
    "        #filename='{epoch:02d}-{loss:.4f}',\n",
    "        filename=\"V2W-BERT\"+args.pretrained+'-{epoch:02d}-{loss:.4f}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_weights_only=True,\n",
    "        #prefix=\"CBERT-\"+args.pretrained,#+'-'+str(args.parallel_mode),\n",
    "        save_last=True,\n",
    "    )\n",
    "    \n",
    "#     if args.check==False:\n",
    "#         args.checkpoint_callback = False\n",
    "#     elif args.parallel_mode=='dp':\n",
    "#         args.callbacks=[checkpoint_callback]        \n",
    "#     else:\n",
    "#         args.checkpoint_callback = False\n",
    "\n",
    "    args.checkpoint_callback = False\n",
    "    \n",
    "    trainer = pl.Trainer.from_argparse_args(args, \n",
    "                                            gpus=gpus,\n",
    "                                            num_nodes=args.nodes, \n",
    "                                            accelerator=args.parallel_mode,\n",
    "                                            max_epochs=args.epochs, \n",
    "                                            gradient_clip_val=1.0,                                            \n",
    "                                            logger=False,\n",
    "                                            progress_bar_refresh_rate=args.refresh_rate,\n",
    "                                            profiler='simple', #'simple',\n",
    "                                            default_root_dir=MODEL_SAVE_DIR,                                            \n",
    "                                            deterministic=True,\n",
    "                                           )\n",
    "\n",
    "    return trainer, dataProcessor, args, dict_args\n",
    "\n",
    "# trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "# next(iter(dataProcessor.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():    \n",
    "    trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "    \n",
    "    model = Model(**dict_args)    \n",
    "    \n",
    "#     printModelParams(model)\n",
    "#     args.early_stop_callback = EarlyStopping('val_loss')\n",
    "    \n",
    "    \n",
    "    print(\"Original weights: \");print_model_value(model)\n",
    "    \n",
    "    t0=time.time()\n",
    "    trainer.fit(model, dataProcessor)\n",
    "    print('Training took: ',time.time()-t0)\n",
    "    \n",
    "    print(\"Trained weights: \");print_model_value(model)\n",
    "    \n",
    "    #if args.parallel_mode!='dp':    \n",
    "    print(\"Saving the last model\")\n",
    "    #MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+\".ckpt\"\n",
    "    MODEL_NAME=MODEL_SAVE_DIR+\"V2WBERT-\"+args.pretrained+\".ckpt\"\n",
    "    trainer.save_checkpoint(MODEL_NAME)\n",
    "\n",
    "    print(\"Testing:....\")\n",
    "    trainer.test(model, dataProcessor.test_dataloader())\n",
    "    \n",
    "    print(\"Training Phase Complete......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "    \n",
    "    #MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+\".ckpt\"    \n",
    "    MODEL_NAME=MODEL_SAVE_DIR+\"V2WBERT-\"+args.pretrained+\".ckpt\"    \n",
    "#     if args.parallel_mode=='dp':\n",
    "#         MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+\"-last.ckpt\"\n",
    "    \n",
    "    if os.path.exists(MODEL_NAME): \n",
    "        print('Loading Saved Model: ',MODEL_NAME)        \n",
    "    else: \n",
    "        print(\"File not found: \",MODEL_NAME)\n",
    "        return\n",
    "    \n",
    "    model=None\n",
    "    \n",
    "    if args.parallel_mode!='dp':\n",
    "        model = Model.load_from_checkpoint(MODEL_NAME)\n",
    "    else:\n",
    "        model = Model(**dict_args)\n",
    "        print(\"Original weights: \");print_model_value(model)\n",
    "        checkpoint = torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    print(\"Loaded weights: \");print_model_value(model)    \n",
    "    trainer.test(model, dataProcessor.test_dataloader())    \n",
    "    print(\"Test Complete......\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "BATCH SIZE:  32\n",
      "PRETRAINED:distilbert-base-uncased\n",
      "Tokenizer: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "Temporal Partition:--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING GPUS: -1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | DistilBertForMaskedLM | 67.0 M\n",
      "------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.942   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "Original weights: \n",
      "model.vocab_projector.bias tensor([-0.5429, -0.6409, -0.6049, -0.6023], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa698be55d94ace9c381eb9007740a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  559.51         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  556.12         \t|1              \t|  556.12         \t|  99.394         \t|\n",
      "run_training_batch                 \t|  0.44919        \t|1182           \t|  530.94         \t|  94.894         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.4483         \t|1182           \t|  529.89         \t|  94.705         \t|\n",
      "training_step_and_backward         \t|  0.4355         \t|1182           \t|  514.76         \t|  92.002         \t|\n",
      "backward                           \t|  0.39836        \t|1182           \t|  470.86         \t|  84.155         \t|\n",
      "model_forward                      \t|  0.034457       \t|1182           \t|  40.729         \t|  7.2793         \t|\n",
      "training_step                      \t|  0.033757       \t|1182           \t|  39.901         \t|  7.1314         \t|\n",
      "evaluation_step_and_end            \t|  0.12966        \t|87             \t|  11.281         \t|  2.0161         \t|\n",
      "validation_step                    \t|  0.12911        \t|87             \t|  11.233         \t|  2.0076         \t|\n",
      "get_train_batch                    \t|  0.0045502      \t|1182           \t|  5.3784         \t|  0.96126        \t|\n",
      "on_train_batch_end                 \t|  0.0030314      \t|1182           \t|  3.5831         \t|  0.6404         \t|\n",
      "cache_result                       \t|  8.2529e-05     \t|5012           \t|  0.41364        \t|  0.073928       \t|\n",
      "on_validation_batch_end            \t|  0.003743       \t|87             \t|  0.32564        \t|  0.0582         \t|\n",
      "training_step_end                  \t|  9.5105e-05     \t|1182           \t|  0.11241        \t|  0.020091       \t|\n",
      "on_train_start                     \t|  0.099109       \t|1              \t|  0.099109       \t|  0.017714       \t|\n",
      "on_after_backward                  \t|  5.1196e-05     \t|1182           \t|  0.060514       \t|  0.010816       \t|\n",
      "on_batch_start                     \t|  3.4283e-05     \t|1182           \t|  0.040523       \t|  0.0072426      \t|\n",
      "on_batch_end                       \t|  2.8262e-05     \t|1182           \t|  0.033405       \t|  0.0059704      \t|\n",
      "on_before_zero_grad                \t|  2.632e-05      \t|1182           \t|  0.03111        \t|  0.0055602      \t|\n",
      "on_train_batch_start               \t|  2.531e-05      \t|1182           \t|  0.029916       \t|  0.0053468      \t|\n",
      "on_validation_start                \t|  0.011158       \t|2              \t|  0.022315       \t|  0.0039883      \t|\n",
      "validation_step_end                \t|  0.00012664     \t|87             \t|  0.011017       \t|  0.0019691      \t|\n",
      "on_validation_batch_start          \t|  4.2983e-05     \t|87             \t|  0.0037395      \t|  0.00066835     \t|\n",
      "on_validation_end                  \t|  0.001568       \t|2              \t|  0.003136       \t|  0.00056049     \t|\n",
      "on_train_epoch_start               \t|  0.0015207      \t|1              \t|  0.0015207      \t|  0.0002718      \t|\n",
      "on_train_end                       \t|  0.0006147      \t|1              \t|  0.0006147      \t|  0.00010986     \t|\n",
      "on_train_epoch_end                 \t|  0.00018654     \t|1              \t|  0.00018654     \t|  3.334e-05      \t|\n",
      "on_epoch_end                       \t|  1.5693e-05     \t|3              \t|  4.708e-05      \t|  8.4145e-06     \t|\n",
      "on_validation_epoch_end            \t|  2.0064e-05     \t|2              \t|  4.0128e-05     \t|  7.1719e-06     \t|\n",
      "on_epoch_start                     \t|  1.2512e-05     \t|3              \t|  3.7537e-05     \t|  6.7089e-06     \t|\n",
      "on_validation_epoch_start          \t|  1.0934e-05     \t|2              \t|  2.1868e-05     \t|  3.9085e-06     \t|\n",
      "on_fit_start                       \t|  1.8629e-05     \t|1              \t|  1.8629e-05     \t|  3.3295e-06     \t|\n",
      "on_train_dataloader                \t|  1.5714e-05     \t|1              \t|  1.5714e-05     \t|  2.8085e-06     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.0569e-05     \t|1              \t|  1.0569e-05     \t|  1.8889e-06     \t|\n",
      "on_val_dataloader                  \t|  9.1949e-06     \t|1              \t|  9.1949e-06     \t|  1.6434e-06     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  559.1828918457031\n",
      "Trained weights: \n",
      "model.vocab_projector.bias tensor([-0.5472, -0.6464, -0.6093, -0.6107], grad_fn=<SliceBackward>)\n",
      "Saving the last model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c35ce2c3c5429b95d9f6ca0a2e16f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  631.21         \t|  100 %          \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  556.12         \t|1              \t|  556.12         \t|  88.105         \t|\n",
      "run_training_batch                 \t|  0.44919        \t|1182           \t|  530.94         \t|  84.116         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.4483         \t|1182           \t|  529.89         \t|  83.948         \t|\n",
      "training_step_and_backward         \t|  0.4355         \t|1182           \t|  514.76         \t|  81.552         \t|\n",
      "backward                           \t|  0.39836        \t|1182           \t|  470.86         \t|  74.597         \t|\n",
      "evaluation_step_and_end            \t|  0.12176        \t|611            \t|  74.394         \t|  11.786         \t|\n",
      "test_step                          \t|  0.11991        \t|524            \t|  62.831         \t|  9.9541         \t|\n",
      "model_forward                      \t|  0.034457       \t|1182           \t|  40.729         \t|  6.4525         \t|\n",
      "training_step                      \t|  0.033757       \t|1182           \t|  39.901         \t|  6.3214         \t|\n",
      "validation_step                    \t|  0.12911        \t|87             \t|  11.233         \t|  1.7796         \t|\n",
      "get_train_batch                    \t|  0.0045502      \t|1182           \t|  5.3784         \t|  0.85208        \t|\n",
      "on_train_batch_end                 \t|  0.0030314      \t|1182           \t|  3.5831         \t|  0.56766        \t|\n",
      "on_test_batch_end                  \t|  0.0016099      \t|524            \t|  0.84361        \t|  0.13365        \t|\n",
      "cache_result                       \t|  8.2042e-05     \t|6594           \t|  0.54099        \t|  0.085707       \t|\n",
      "on_validation_batch_end            \t|  0.003743       \t|87             \t|  0.32564        \t|  0.05159        \t|\n",
      "training_step_end                  \t|  9.5105e-05     \t|1182           \t|  0.11241        \t|  0.017809       \t|\n",
      "on_train_start                     \t|  0.099109       \t|1              \t|  0.099109       \t|  0.015702       \t|\n",
      "test_step_end                      \t|  0.00012953     \t|524            \t|  0.067871       \t|  0.010753       \t|\n",
      "on_after_backward                  \t|  5.1196e-05     \t|1182           \t|  0.060514       \t|  0.0095871      \t|\n",
      "on_batch_start                     \t|  3.4283e-05     \t|1182           \t|  0.040523       \t|  0.0064199      \t|\n",
      "on_batch_end                       \t|  2.8262e-05     \t|1182           \t|  0.033405       \t|  0.0052923      \t|\n",
      "on_before_zero_grad                \t|  2.632e-05      \t|1182           \t|  0.03111        \t|  0.0049286      \t|\n",
      "on_train_batch_start               \t|  2.531e-05      \t|1182           \t|  0.029916       \t|  0.0047395      \t|\n",
      "on_test_batch_start                \t|  4.317e-05      \t|524            \t|  0.022621       \t|  0.0035838      \t|\n",
      "on_validation_start                \t|  0.011158       \t|2              \t|  0.022315       \t|  0.0035354      \t|\n",
      "on_test_start                      \t|  0.018594       \t|1              \t|  0.018594       \t|  0.0029459      \t|\n",
      "validation_step_end                \t|  0.00012664     \t|87             \t|  0.011017       \t|  0.0017455      \t|\n",
      "on_validation_batch_start          \t|  4.2983e-05     \t|87             \t|  0.0037395      \t|  0.00059243     \t|\n",
      "on_validation_end                  \t|  0.001568       \t|2              \t|  0.003136       \t|  0.00049683     \t|\n",
      "on_train_epoch_start               \t|  0.0015207      \t|1              \t|  0.0015207      \t|  0.00024093     \t|\n",
      "on_test_end                        \t|  0.00074983     \t|1              \t|  0.00074983     \t|  0.00011879     \t|\n",
      "on_train_end                       \t|  0.0006147      \t|1              \t|  0.0006147      \t|  9.7386e-05     \t|\n",
      "on_train_epoch_end                 \t|  0.00018654     \t|1              \t|  0.00018654     \t|  2.9553e-05     \t|\n",
      "on_epoch_end                       \t|  1.4435e-05     \t|4              \t|  5.7739e-05     \t|  9.1474e-06     \t|\n",
      "on_epoch_start                     \t|  1.2419e-05     \t|4              \t|  4.9678e-05     \t|  7.8703e-06     \t|\n",
      "on_validation_epoch_end            \t|  2.0064e-05     \t|2              \t|  4.0128e-05     \t|  6.3573e-06     \t|\n",
      "on_fit_end                         \t|  3.5654e-05     \t|1              \t|  3.5654e-05     \t|  5.6485e-06     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.7455e-05     \t|2              \t|  3.4911e-05     \t|  5.5308e-06     \t|\n",
      "on_test_epoch_end                  \t|  3.2264e-05     \t|1              \t|  3.2264e-05     \t|  5.1115e-06     \t|\n",
      "on_validation_epoch_start          \t|  1.0934e-05     \t|2              \t|  2.1868e-05     \t|  3.4645e-06     \t|\n",
      "on_fit_start                       \t|  1.8629e-05     \t|1              \t|  1.8629e-05     \t|  2.9514e-06     \t|\n",
      "on_train_dataloader                \t|  1.5714e-05     \t|1              \t|  1.5714e-05     \t|  2.4896e-06     \t|\n",
      "on_test_epoch_start                \t|  1.4795e-05     \t|1              \t|  1.4795e-05     \t|  2.3439e-06     \t|\n",
      "on_test_dataloader                 \t|  9.3989e-06     \t|1              \t|  9.3989e-06     \t|  1.489e-06      \t|\n",
      "on_val_dataloader                  \t|  9.1949e-06     \t|1              \t|  9.1949e-06     \t|  1.4567e-06     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1.845301866531372}\n",
      "--------------------------------------------------------------------------------\n",
      "Training Phase Complete......\n",
      "--------------------------------------------------\n",
      "BATCH SIZE:  32\n",
      "PRETRAINED:distilbert-base-uncased\n",
      "Tokenizer: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "Temporal Partition:--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING GPUS: -1\n",
      "--------------------------------------------------\n",
      "Loading Saved Model:  ./Results/Model/V2WBERT-distilbert-base-uncased.ckpt\n",
      "Model:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "Original weights: \n",
      "model.vocab_projector.bias tensor([-0.5429, -0.6409, -0.6049, -0.6023], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights: \n",
      "model.vocab_projector.bias tensor([-0.5472, -0.6464, -0.6093, -0.6107], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5893c31387f8438a98a36b2437e4f66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  69.917         \t|  100 %          \t|\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "evaluation_step_and_end            \t|  0.11983        \t|524            \t|  62.792         \t|  89.81          \t|\n",
      "test_step                          \t|  0.11931        \t|524            \t|  62.516         \t|  89.415         \t|\n",
      "on_test_batch_end                  \t|  0.0015602      \t|524            \t|  0.81756        \t|  1.1693         \t|\n",
      "cache_result                       \t|  7.8707e-05     \t|1580           \t|  0.12436        \t|  0.17786        \t|\n",
      "test_step_end                      \t|  0.00012641     \t|524            \t|  0.066239       \t|  0.09474        \t|\n",
      "on_test_batch_start                \t|  4.2411e-05     \t|524            \t|  0.022224       \t|  0.031786       \t|\n",
      "on_test_start                      \t|  0.021323       \t|1              \t|  0.021323       \t|  0.030497       \t|\n",
      "on_test_end                        \t|  0.00098096     \t|1              \t|  0.00098096     \t|  0.001403       \t|\n",
      "on_epoch_start                     \t|  3.0208e-05     \t|1              \t|  3.0208e-05     \t|  4.3206e-05     \t|\n",
      "on_test_dataloader                 \t|  2.3535e-05     \t|1              \t|  2.3535e-05     \t|  3.3661e-05     \t|\n",
      "on_test_epoch_end                  \t|  2.0081e-05     \t|1              \t|  2.0081e-05     \t|  2.8722e-05     \t|\n",
      "on_test_epoch_start                \t|  1.5716e-05     \t|1              \t|  1.5716e-05     \t|  2.2478e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.2911e-05     \t|1              \t|  1.2911e-05     \t|  1.8466e-05     \t|\n",
      "on_epoch_end                       \t|  1.2212e-05     \t|1              \t|  1.2212e-05     \t|  1.7467e-05     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1.8468996286392212}\n",
      "--------------------------------------------------------------------------------\n",
      "Test Complete......\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    test_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
