{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# // ***********************************************************************\n",
    "# //\n",
    "# //        V2W-BERT: A Python library for vulnerability classification\n",
    "# //               Siddhartha Das (das90@purdue.edu) : Purdue University\n",
    "# //               Mahantesh Halappanavar (hala@pnnl.gov): Pacific Northwest National Laboratory   \n",
    "# //\n",
    "# // ***********************************************************************\n",
    "\n",
    " \n",
    "# Copyright Â© 2022, Battelle Memorial Institute\n",
    "# All rights reserved.\n",
    "\n",
    " \n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    " \n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "\n",
    "#    list of conditions and the following disclaimer.\n",
    "\n",
    " \n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "\n",
    "#    this list of conditions and the following disclaimer in the documentation\n",
    "\n",
    "#    and/or other materials provided with the distribution.\n",
    "\n",
    " \n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  24\n"
     ]
    }
   ],
   "source": [
    "import torch# If there's a GPU available...\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading directory:  ./Results/\n",
      "Model Saving directory: ./Results/Model/\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Dataset import getDataset, getDummyDataset, getRandomDataset, Data        \n",
    "\n",
    "DIR='./Results/'\n",
    "    \n",
    "from pathlib import Path\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_LOAD_DIR=\"./Dataset/NVD/processed/\"\n",
    "MODEL_SAVE_DIR=DIR+'Model/'\n",
    "\n",
    "Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Data loading directory: \", DIR)\n",
    "print(\"Model Saving directory:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import zipfile\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, CosineEmbeddingLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelWithLMHead\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "from transformers.models.roberta.modeling_roberta import RobertaLMHead\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.trainer.supporters import CombinedLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "from random import sample\n",
    "\n",
    "seed_val = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "try:\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except:\n",
    "    print(\"nothing to set for cudnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy precision level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEG_LINKS=None\n",
    "TOP_K0=[1,3,5]\n",
    "TOP_K1=[1,2,2]\n",
    "TOP_K2=[1,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(filename,infos,append=True):    \n",
    "    f=None\n",
    "    if append==True:  \n",
    "        f=open(filename, 'a+')\n",
    "    else:\n",
    "        f=open(filename, 'w')\n",
    "    \n",
    "    f.write(\"\\n\")    \n",
    "    for key, values in infos.items():\n",
    "        f.write(\"%s :\" % key)\n",
    "        if type(values).__name__=='list':            \n",
    "            for item in values:\n",
    "                f.write(\"%s \" % item)\n",
    "        else:\n",
    "            f.write(\"%s \" % values)           \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.close()\n",
    "#log_results(\"testing\",{'train_acc':[1,2,3,4]})\n",
    "#log_results(\"testing\",{'train_acc':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CVE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, org_labels, collator, data, k_neg_link=10, use_collator=True):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.org_labels = org_labels\n",
    "        self.collator = collator\n",
    "        self.data = data\n",
    "        self.k_neg_link=k_neg_link\n",
    "        self.use_collator=use_collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        if self.use_collator:\n",
    "            item = self.collator([item])\n",
    "            item = {key: val[0] for key, val in item.items()}\n",
    "        item['h_labels']= self.labels[idx]\n",
    "        item['o_labels']= self.org_labels[idx]\n",
    "        \n",
    "        POS=[]\n",
    "        NEG=[]\n",
    "        \n",
    "        #------POSITIVE LINKS-----\n",
    "        p_labels=(self.labels[idx]==1).nonzero().flatten()        \n",
    "        for c in p_labels: POS.append(c.item())\n",
    "        \n",
    "        #------NEG LINKS-----\n",
    "        n_labels=(self.labels[idx]==0).nonzero().flatten()    \n",
    "        limit=min(len(n_labels),self.k_neg_link)\n",
    "        indexs=np.random.choice(len(n_labels),limit, replace=False)\n",
    "        n_labels=n_labels[indexs]            \n",
    "        for c in n_labels: NEG.append(c.item())\n",
    "        \n",
    "        NEG=(NEG*int(self.k_neg_link/len(NEG)+1))[:self.k_neg_link]\n",
    "        POS=(POS*int(self.k_neg_link/len(POS)+1))[:self.k_neg_link]\n",
    "        \n",
    "        item['pos']=torch.tensor(POS, dtype=torch.long)\n",
    "        item['neg']=torch.tensor(NEG, dtype=torch.long)\n",
    "        \n",
    "        item['pos_label']=torch.ones(len(POS), dtype=torch.long)\n",
    "        item['neg_label']=torch.zeros(len(NEG), dtype=torch.long)\n",
    "             \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing CVE dataset\n",
    "```\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "if type(labels)!=torch.Tensor:\n",
    "    labels=torch.tensor(labels,dtype=torch.long)\n",
    "else:\n",
    "    labels=labels.type(torch.LongTensor)\n",
    "    \n",
    "train_encodings = berttokenizer(sentences, truncation=True, padding=True)\n",
    "dataset = CDataset(train_encodings, labels, labels, datacollator, data)\n",
    "\n",
    "dataset[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CWE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, org_labels, collator, data, use_collator=True):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.org_labels = org_labels\n",
    "        self.collator = collator\n",
    "        self.data = data\n",
    "        self.use_collator=use_collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        if self.use_collator:\n",
    "            item = self.collator([item])\n",
    "            item = {key: val[0] for key, val in item.items()}\n",
    "        item['h_labels']= self.labels[idx]\n",
    "        item['o_labels']= self.org_labels[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing CWE Dataset\n",
    "```\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "if type(labels)!=torch.Tensor:\n",
    "    labels=torch.tensor(labels,dtype=torch.long)\n",
    "else:\n",
    "    labels=labels.type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "class_mask=(data.class_mask == True).nonzero().flatten().numpy()\n",
    "class_encodings = berttokenizer([sentences[i] for i in class_mask], truncation=True, padding=True)\n",
    "\n",
    "dataset = CDataset(class_encodings, labels, labels, datacollator, data)\n",
    "\n",
    "dataset[3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.save_hyperparameters()\n",
    "        if isinstance(args, tuple): args = args[0]\n",
    "        self.hparams = args\n",
    "        self.batch_size=self.hparams.batch_size        \n",
    "\n",
    "        print(f'PRETRAINED:{self.hparams.pretrained}')\n",
    "\n",
    "        A = AutoTokenizer\n",
    "        self.tokenizer = A.from_pretrained(self.hparams.pretrained, use_fast=True)\n",
    "        print('Tokenizer:', type(self.tokenizer))\n",
    "        \n",
    "        self.datacollator=DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=0.15)\n",
    "    \n",
    "    \n",
    "    def get_cwe_level(self):\n",
    "        self.levels={}\n",
    "        for key,values in self.data.depth.items():\n",
    "            if type(values)==int:\n",
    "                values=[values]\n",
    "\n",
    "            for val in values:\n",
    "                if val in self.levels:\n",
    "                    if(key not in self.levels[val]):self.levels[val].append(key)\n",
    "                else:\n",
    "                    self.levels[val]=[key]\n",
    "\n",
    "#         for i,j in self.levels.items():\n",
    "#             print(i,\"->\",len(j),':',j)\n",
    "            \n",
    "    \n",
    "    def indexsToUpdate(self,parentid,indexs):\n",
    "        if parentid==-1:\n",
    "            return\n",
    "\n",
    "        if(parentid not in indexs):\n",
    "            indexs.append(parentid)\n",
    "\n",
    "        parents=self.data.child_parent[parentid]\n",
    "\n",
    "        for parent in parents:\n",
    "                self.indexsToUpdate(parent,indexs)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def updates_label_by_hierarchy(self,labels):\n",
    "        all_mask=self.data.train_mask|self.data.val_mask|self.data.test_mask|self.data.class_mask\n",
    "        labeled_indexs= (all_mask == True).nonzero().flatten().numpy()\n",
    "\n",
    "        for i in labeled_indexs:    \n",
    "            row_labels=(labels[i]==1).nonzero().flatten().numpy()    \n",
    "\n",
    "            #print(row_labels,\"->\",end='')\n",
    "\n",
    "            indexs=[]\n",
    "            for r_label in row_labels:\n",
    "                \n",
    "                if r_label not in self.data.child_parent:\n",
    "                    parents=[-1]\n",
    "                else:\n",
    "                    parents=self.data.child_parent[r_label]\n",
    "\n",
    "                for parent in parents:\n",
    "                    self.indexsToUpdate(parent,indexs)\n",
    "\n",
    "            #print(indexs)\n",
    "\n",
    "            labels[i][indexs]=1\n",
    "\n",
    "        return labels\n",
    "               \n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        MAX_TEXT_LENGTH=512\n",
    "    \n",
    "#         CVE dataset\n",
    "#         ------------------------------------        \n",
    "        data, df_CVE, df_CWE=None,None,None\n",
    "    \n",
    "        if self.hparams.rand_dataset=='dummy':            \n",
    "            #------------------------------------\n",
    "            data, sentences, labels = getDummyDataset()        \n",
    "            #------------------------------------\n",
    "        else:        \n",
    "            if self.hparams.rand_dataset=='temporal':\n",
    "                print(\"Temporal Partition:--\")\n",
    "                data, df_CVE, df_CWE = getDataset(DATASET_LOAD_DIR)            \n",
    "            else:\n",
    "                print(\"Random Partition:--\")\n",
    "                data, df_CVE, df_CWE = getRandomDataset(DATASET_LOAD_DIR, 0.70, 0.10)\n",
    "\n",
    "            sentences=df_CVE['CVE Description'].apply(lambda x: str(x)[:MAX_TEXT_LENGTH])\n",
    "            labels=data.y\n",
    "            CWE_IDS_USED=df_CWE['Name'].tolist()\n",
    "            INDEX_TO_CWE_MAP=dict(zip(list(range(len(CWE_IDS_USED))),CWE_IDS_USED))\n",
    "            CWE_TO_INDEX_MAP=dict(zip(CWE_IDS_USED,list(range(len(CWE_IDS_USED)))))\n",
    "            sentences=sentences.tolist()\n",
    "        \n",
    "#         #------------------------------------        \n",
    "#         #CVE dummy dataset\n",
    "#         #------------------------------------\n",
    "#         data, sentences, labels = getDummyDataset()        \n",
    "#         #------------------------------------\n",
    "\n",
    "        \n",
    "        \n",
    "        if type(labels)!=torch.Tensor:\n",
    "            labels=torch.tensor(labels,dtype=torch.long)\n",
    "        else:\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "        self.data=data        \n",
    "        org_labels=np.copy(labels) ###keep the copy of orgiginal labels\n",
    "        \n",
    "        self.get_cwe_level()        \n",
    "        labels=self.updates_label_by_hierarchy(labels)\n",
    "\n",
    "        \n",
    "        self.NUM_CLASSES=len(data.y[0])\n",
    "        \n",
    "        train_mask= (data.train_mask == True).nonzero().flatten().numpy()\n",
    "        val_mask= (data.val_mask == True).nonzero().flatten().numpy()\n",
    "        test_mask= (data.test_mask == True).nonzero().flatten().numpy()\n",
    "        class_mask=(data.class_mask == True).nonzero().flatten().numpy()\n",
    "        \n",
    "        print(\"Train size:\", len(train_mask))\n",
    "        print(\"Val size:\", len(val_mask))\n",
    "        print(\"Test size:\", len(test_mask))\n",
    "        print(\"Class size:\",len(class_mask))\n",
    "\n",
    "        \n",
    "        train_encodings = self.tokenizer([sentences[i] for i in train_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        \n",
    "        self.dataset = CDataset(\n",
    "            train_encodings, \n",
    "            labels[data.train_mask], \n",
    "            org_labels[data.train_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.datasetNC = CDataset(\n",
    "            train_encodings, \n",
    "            labels[data.train_mask], \n",
    "            org_labels[data.train_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )        \n",
    "                \n",
    "        val_encodings = self.tokenizer([sentences[i] for i in val_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.val_dataset=CDataset(\n",
    "            val_encodings, \n",
    "            labels[data.val_mask],\n",
    "            org_labels[data.val_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.val_datasetNC=CDataset(\n",
    "            val_encodings, \n",
    "            labels[data.val_mask],\n",
    "            org_labels[data.val_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )\n",
    "        \n",
    "        test_encodings = self.tokenizer([sentences[i] for i in test_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.test_dataset=CDataset(\n",
    "            test_encodings,\n",
    "            labels[data.test_mask],\n",
    "            org_labels[data.test_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.test_datasetNC=CDataset(\n",
    "            test_encodings,\n",
    "            labels[data.test_mask],\n",
    "            org_labels[data.test_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )\n",
    "        \n",
    "        class_encodings = self.tokenizer([sentences[i] for i in class_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        self.class_dataset = DDataset(\n",
    "            class_encodings, \n",
    "            labels[data.class_mask], \n",
    "            org_labels[data.class_mask],\n",
    "            self.datacollator,\n",
    "            data\n",
    "        )\n",
    "        self.class_datasetNC = DDataset(\n",
    "            class_encodings, \n",
    "            labels[data.class_mask], \n",
    "            org_labels[data.class_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            use_collator=False\n",
    "        )\n",
    "    \n",
    "    def class_dataloader(self, use_collator=True):\n",
    "        dataset=None\n",
    "        if use_collator:\n",
    "            dataset=self.class_dataset\n",
    "        else:\n",
    "            dataset=self.class_datasetNC\n",
    "            \n",
    "        class_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        loader_cwe = DataLoader(\n",
    "            dataset,\n",
    "            sampler=class_sampler, \n",
    "            batch_size=len(dataset),\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size, NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cwe\n",
    "    \n",
    "    def train_dataloader(self,use_collator=True):\n",
    "        \n",
    "        dataset=None\n",
    "        if use_collator:\n",
    "            dataset=self.dataset\n",
    "        else:\n",
    "            dataset=self.datasetNC\n",
    "        \n",
    "        train_sampler = RandomSampler(dataset)        \n",
    "        loader_cve = DataLoader(\n",
    "            dataset,\n",
    "            sampler=train_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size,NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "    \n",
    "    def val_dataloader(self, use_collator=True):\n",
    "        dataset=None\n",
    "        \n",
    "        if use_collator:\n",
    "            dataset=self.val_dataset\n",
    "        else:\n",
    "            dataset=self.val_datasetNC\n",
    "    \n",
    "        val_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        loader_cve=DataLoader(\n",
    "            dataset,\n",
    "            sampler=val_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size,NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "    \n",
    "    def test_dataloader(self,use_collator=True):\n",
    "        dataset=None        \n",
    "        if use_collator:\n",
    "            dataset=self.test_dataset\n",
    "        else:\n",
    "            dataset=self.test_datasetNC\n",
    "\n",
    "        test_sampler = SequentialSampler(dataset)        \n",
    "        loader_cve=DataLoader(\n",
    "            dataset,\n",
    "            sampler=test_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size, NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "\n",
    "\n",
    "# #----------------------------\n",
    "# import argparse\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('--rand_dataset', type=str, default=\"temporal\", choices=['temporal','random','dummy'])\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# parser = pl.Trainer.add_argparse_args(parser)\n",
    "# args = parser.parse_args()\n",
    "# dataProcessor = DataProcessing(args)\n",
    "# dataProcessor.setup()\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "    parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\")    \n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--nr_frozen_epochs', type=int, default=5)\n",
    "    #parser.add_argument('--training_portion', type=float, default=0.9)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--frac', type=float, default=1)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--nodes', type=int, default=1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--refresh_rate', type=int, default=100)\n",
    "    parser.add_argument('--checkpointing', type=str, default='True', choices=['True','False'])\n",
    "    parser.add_argument('--use_rd', type=str, default='False', choices=['True','False'])\n",
    "    parser.add_argument('--lm_lambda', type=float, default=0.1)\n",
    "    parser.add_argument('--rand_dataset', type=str, default=\"temporal\", choices=['temporal','random','dummy'])\n",
    "    parser.add_argument('--use_pretrained', type=str, default='True', choices=['True','False'])\n",
    "    parser.add_argument('--neg_link', type=int, default=120)\n",
    "    parser.add_argument('--check', type=bool, default=False)\n",
    "    parser.add_argument('--performance_mode', type=str, default='False', choices=['True','False'])\n",
    "    parser.add_argument('--freeze', type=str, default='True')\n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    # parser = Model.add_model_specific_args(parser) parser = Data.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    global K_NEG_LINKS\n",
    "    K_NEG_LINKS=args.neg_link    \n",
    "    print(\"-\"*50)\n",
    "    print(\"POS/NEG Links: \",K_NEG_LINKS)\n",
    "    print(\"BATCH SIZE: \", args.batch_size)\n",
    "    \n",
    "    \n",
    "    # start : get training steps\n",
    "    dataProcessor = DataProcessing(args)\n",
    "    dataProcessor.setup()\n",
    "    args.NUM_CLASSES=dataProcessor.NUM_CLASSES\n",
    "    args.MODEL_NAME=\"V2WBERT-LINK-\"+args.pretrained+'-'+args.parallel_mode\n",
    "    args.MODEL_DIR_FILE=MODEL_SAVE_DIR+args.MODEL_NAME\n",
    "    #args.PRE_TRAINED_MODEL=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+'.ckpt'\n",
    "    args.PRE_TRAINED_MODEL=MODEL_SAVE_DIR+\"V2WBERT-\"+args.pretrained+'.ckpt'\n",
    "    \n",
    "    args.num_training_steps = len(dataProcessor.train_dataloader())*args.epochs\n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    gpus=-1\n",
    "    if NUM_GPUS>0:\n",
    "        gpus=args.num_gpus\n",
    "    else:\n",
    "        gpus=None\n",
    "        args.parallel_mode=None\n",
    "    \n",
    "    print(\"USING GPUS:\", gpus)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='loss_epoch',\n",
    "        dirpath=MODEL_SAVE_DIR,\n",
    "        #filename='{epoch:02d}-{loss:.4f}',\n",
    "        filename=\"V2WBERT-LINK\"+args.pretrained+'-{epoch:02d}-{loss:.4f}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_weights_only=True,\n",
    "        #prefix=\"CBERT-LINK\"+args.pretrained+'-'+str(args.parallel_mode),\n",
    "        save_last=True,\n",
    "    )\n",
    "    \n",
    "    if args.check==False:\n",
    "        args.checkpoint_callback = False\n",
    "    elif args.parallel_mode=='dp':\n",
    "        args.callbacks=[checkpoint_callback]        \n",
    "    else:\n",
    "        args.checkpoint_callback = False\n",
    "    \n",
    "    trainer = pl.Trainer.from_argparse_args(args, \n",
    "                                            gpus=gpus,\n",
    "                                            num_nodes=args.nodes, \n",
    "                                            accelerator=args.parallel_mode,\n",
    "                                            max_epochs=args.epochs, \n",
    "                                            gradient_clip_val=1.0,                                            \n",
    "                                            logger=False,\n",
    "                                            progress_bar_refresh_rate=args.refresh_rate,\n",
    "                                            profiler=False, #'simple',\n",
    "                                            default_root_dir=MODEL_SAVE_DIR,                                            \n",
    "                                            deterministic=True,\n",
    "                                           )\n",
    "    \n",
    "    return trainer, dataProcessor, args, dict_args\n",
    "\n",
    "#trainer, dataProcessor, args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForMaskedLM\n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification    \n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaForMaskedLM\n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/distilbert/modeling_distilbert.html#DistilBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import gelu\n",
    "\n",
    "class BaseModelDistillBert(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self._frozen = False\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,                                            \n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        self.config=config\n",
    "        \n",
    "        A = AutoModel \n",
    "        self.base_model = A.from_pretrained(self.hparams.pretrained, config=config)                \n",
    "                \n",
    "        self.vocab_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "        \n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "\n",
    "        print('Base: ', type(self.base_model))\n",
    "    \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )        \n",
    "        \n",
    "        hidden_states = outputs[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "        masked_lm_loss=None\n",
    "        if 'labels' in batch:        \n",
    "            labels=batch['labels']      \n",
    "            \n",
    "            prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "            prediction_logits = gelu(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "\n",
    "            masked_lm_loss = self.CELoss(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            del labels, prediction_logits\n",
    "        \n",
    "        \n",
    "        pooled_output = hidden_states[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled = self.dropout(pooled_output)  # (bs, dim)\n",
    "\n",
    "        del batch\n",
    "        del outputs\n",
    "        \n",
    "        if masked_lm_loss is not None: \n",
    "            masked_lm_loss = masked_lm_loss.view(1)\n",
    "        \n",
    "        return (masked_lm_loss, pooled)\n",
    "        \n",
    "    \n",
    "# #----------------------------\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# args = parser.parse_args()\n",
    "# dict_args = vars(args)\n",
    "# base_model=BaseModelDistillBert(**dict_args)\n",
    "# print(base_model)\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self._frozen = False\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,                                            \n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        self.config=config\n",
    "        \n",
    "        A = AutoModel #AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "        self.base_model = A.from_pretrained(self.hparams.pretrained, config=config)                \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        if self.hparams.pretrained in ['bert-base-uncased','bert-large-uncased']: \n",
    "            self.lm_cls = BertOnlyMLMHead(config)\n",
    "                    \n",
    "        elif self.hparams.pretrained in ['roberta-base','roberta-large']:\n",
    "            self.lm_cls = RobertaLMHead(config)\n",
    "                    \n",
    "        print(\"LM: \",type(self.lm_cls))\n",
    "        \n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "\n",
    "        print('Base: ', type(self.base_model))\n",
    "        \n",
    "        print(\"Freezing Layers:-\")\n",
    "        if self.hparams.freeze=='True':                \n",
    "            if self.hparams.pretrained in ['bert-base-uncased','roberta-base']:\n",
    "                self.freeze('layer.9')\n",
    "            elif self.hparams.pretrained in ['bert-large-uncased', 'roberta-large']:\n",
    "                self.freeze('layer.21')            \n",
    "            else:\n",
    "                print(\"Nothing Frozen\")\n",
    "        elif self.hparams.freeze=='False':\n",
    "            print(\"Nothing Frozen\")\n",
    "        \n",
    "        else:\n",
    "            self.freeze(self.hparams.freeze)\n",
    "                        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )        \n",
    "        \n",
    "        masked_lm_loss=None\n",
    "        if 'labels' in batch:        \n",
    "            labels=batch['labels']      \n",
    "            prediction_scores = self.lm_cls(outputs.last_hidden_state)            \n",
    "            masked_lm_loss = self.CELoss(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            del labels, prediction_scores\n",
    "        \n",
    "        pooled=outputs.pooler_output        \n",
    "        pooled=self.dropout(pooled)\n",
    "        \n",
    "        del batch\n",
    "        del outputs\n",
    "        \n",
    "        if masked_lm_loss is not None: \n",
    "            masked_lm_loss = masked_lm_loss.view(1)\n",
    "        \n",
    "        return (masked_lm_loss, pooled)\n",
    "    \n",
    "    def freeze(self,layername) -> None:        \n",
    "        for name, param in self.base_model.named_parameters():            \n",
    "#             print(name)            \n",
    "            if layername in name:\n",
    "                print(\"Froze upto: \", name)\n",
    "                break\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self._frozen = True\n",
    "        \n",
    "    \n",
    "# #----------------------------\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# args = parser.parse_args()\n",
    "# dict_args = vars(args)\n",
    "# base_model=BaseModel(**dict_args)\n",
    "# print(base_model)\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionModel(pl.LightningModule):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #self.lc_1 = nn.Linear(config.hidden_size*2,2)\n",
    "        #self.lc_1 = nn.Linear(config.hidden_size,2)\n",
    "        \n",
    "        self.lc_1 = nn.Linear(config.hidden_size*2, config.hidden_size)\n",
    "        self.lc_2 = nn.Linear(config.hidden_size,2)\n",
    "        \n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, CVE_vectors, CWE_vectors, true_links=None):\n",
    "        logits = self.lc_1(torch.cat((torch.abs(CVE_vectors-CWE_vectors),CVE_vectors*CWE_vectors), 1))\n",
    "        #logits = self.lc_1(CVE_vectors*CWE_vectors)\n",
    "        logits = self.lc_2(self.tanh(logits))\n",
    "        \n",
    "        loss=None\n",
    "        if true_links is not None:\n",
    "            loss=self.CELoss(logits,true_links)     \n",
    "            \n",
    "        if loss is not None: \n",
    "            loss = loss.view(1)\n",
    "            \n",
    "        return (loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        \n",
    "        if self.hparams.pretrained == \"distilbert-base-uncased\":\n",
    "            self.base_model=BaseModelDistillBert(*args, **kwargs)\n",
    "        else:        \n",
    "            self.base_model=BaseModel(*args, **kwargs)\n",
    "        self.link_model=LinkPredictionModel(self.base_model.config, *args, **kwargs)\n",
    "        \n",
    "        if NUM_GPUS > 1: \n",
    "            ids=None\n",
    "            if self.hparams.num_gpus!=-1:\n",
    "                ids=list(range(self.hparams.num_gpus))\n",
    "                \n",
    "            self.base_model = nn.DataParallel(self.base_model, device_ids=ids)\n",
    "            self.link_model = nn.DataParallel(self.link_model, device_ids=ids)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, CWE_pooled):                \n",
    "        lm_loss, CVE_pooled=model.base_model(batch)\n",
    "        \n",
    "        CVE_vectors=CVE_pooled[batch['CVE_index']]\n",
    "        CWE_vectors=CWE_pooled[batch['CWE_index']]\n",
    "        true_links=batch['true_labels']\n",
    "    \n",
    "        (loss, logits)=self.link_model(CVE_vectors,CWE_vectors, true_links)        \n",
    "\n",
    "        del CVE_vectors, CWE_vectors, batch\n",
    "        \n",
    "        loss=loss.mean()\n",
    "        \n",
    "        if lm_loss is not None:\n",
    "            loss+= ((self.hparams.lm_lambda)*lm_loss.mean())\n",
    "\n",
    "        return (loss, logits, true_links)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.01\n",
    "        }, {\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.0\n",
    "        }]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0, # Default value in run_glue.py\n",
    "            num_training_steps=self.hparams.num_training_steps)\n",
    "\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelParams(model):\n",
    "    print (model)\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "    print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-5:]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "        \n",
    "def print_model_value(model):\n",
    "    params = list(model.named_parameters())\n",
    "    print (params[-1][0],params[-1][1][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Two key aspects:\n",
    "\n",
    "- pytorch lightning can add arguments to the parser automatically\n",
    "- you can manually add your own specific arguments.\n",
    "\n",
    "- there is a little more code than seems necessary, because of a particular argument the scheduler\n",
    "  needs. There is currently an open issue on this complication\n",
    "  https://github.com/PyTorchLightning/pytorch-lightning/issues/1038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "POS/NEG Links:  120\n",
      "BATCH SIZE:  32\n",
      "PRETRAINED:distilbert-base-uncased\n",
      "Tokenizer: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "Temporal Partition:--\n",
      "Train size: 10655\n",
      "Val size: 2719\n",
      "Test size: 16747\n",
      "Class size: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/das90/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory ./Results/Model/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING GPUS: -1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n"
     ]
    }
   ],
   "source": [
    "trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "log_results(args.MODEL_DIR_FILE+'_log.txt',dict_args,append=False)\n",
    "\n",
    "model = Model(**dict_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pretrained Model:  ./Results/Model/V2WBERT-distilbert-base-uncased.ckpt\n",
      "Model:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "Matched:  100\n"
     ]
    }
   ],
   "source": [
    "class PretrainedModel(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        A = AutoModelForMaskedLM\n",
    "        self.model = A.from_pretrained(self.hparams.pretrained, config=config)\n",
    "        print('Model: ', type(self.model))\n",
    "\n",
    "if args.use_pretrained=='True':\n",
    "    print('Loading Pretrained Model: ',args.PRE_TRAINED_MODEL)    \n",
    "    if os.path.exists(args.PRE_TRAINED_MODEL): \n",
    "            \n",
    "        pretrainedModel=PretrainedModel(**dict_args)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            if args.pretrained in ['bert-base-uncased','bert-large-uncased']:        \n",
    "\n",
    "                checkpoint = torch.load(args.PRE_TRAINED_MODEL, map_location=lambda storage, loc: storage)\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.bert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.bert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "\n",
    "            elif args.pretrained in ['roberta-base','roberta-large']:\n",
    "\n",
    "                checkpoint = torch.load(args.PRE_TRAINED_MODEL, map_location=lambda storage, loc: storage)\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.roberta).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.roberta).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "            elif args.pretrained=='distilbert-base-uncased':\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.distilbert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.distilbert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "        except:\n",
    "            print(\"Pytorch version missmatch between saved and new model\")\n",
    "\n",
    "    else:\n",
    "        print(\"File not found will continue with original model\")\n",
    "else:\n",
    "    print(\"Use default model as base...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depolying model to  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Depolying model to \",device)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=dataProcessor.train_dataloader()    \n",
    "val_dataloader=dataProcessor.val_dataloader()\n",
    "test_dataloader=dataProcessor.test_dataloader()\n",
    "class_dataloader=dataProcessor.class_dataloader()\n",
    "\n",
    "train_dataloaderNC=dataProcessor.train_dataloader(use_collator=False)    \n",
    "val_dataloaderNC=dataProcessor.val_dataloader(use_collator=False)\n",
    "test_dataloaderNC=dataProcessor.test_dataloader(use_collator=False)\n",
    "class_dataloaderNC=dataProcessor.class_dataloader(use_collator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_dataloader=iter(class_dataloaderNC)\n",
    "# next(iter_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def th_link_acc(preds,labels,th=0.50):\n",
    "    pred=(preds >= th).astype(int)    \n",
    "    return np.sum(pred == labels) / len(labels)\n",
    "\n",
    "def th_link_f1_score(logits,y_true,th=0.50):\n",
    "    logits=(torch.nn.functional.softmax(logits,dim=1))[:1]\n",
    "    y_pred=(logits >= th).astype(int)    \n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "def link_f1_score(logits,y_true):\n",
    "#     print(logits)\n",
    "#     logits=(torch.nn.functional.softmax(logits,dim=1))\n",
    "    y_pred=np.argmax(logits,axis=1)\n",
    "#     print(y_pred)\n",
    "#     print(y_true)\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "#     f1_score(y_true, y_pred, average='micro')\n",
    "#     f1_score(y_true, y_pred, average='weighted')    \n",
    "#     f1_score(y_true, y_pred, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in dataProcessor.levels.items():\n",
    "#     print(i,\"->\",len(j),':',j)\n",
    "levels=dataProcessor.levels\n",
    "data=dataProcessor.data\n",
    "\n",
    "def getHierIndexs(child, preds,k2, indexs, rec=-1):    \n",
    "    if child in data.parent_child:\n",
    "        children=data.parent_child[child]           \n",
    "        child_preds=preds[children]\n",
    "\n",
    "        k_child=min(len(children),k2)\n",
    "        child_indexs=np.argpartition(child_preds, -k_child)[-k_child:]                \n",
    "        child_index_map=dict(zip(range(len(children)),children))\n",
    "        \n",
    "        grandchildren=[child_index_map[ix] for ix in child_indexs]\n",
    "        \n",
    "        indexs.extend(grandchildren)\n",
    "    \n",
    "        if(rec==-1):\n",
    "            for grandchild in grandchildren:\n",
    "                getHierIndexs(grandchild,preds,k2, indexs)\n",
    "    \n",
    "\n",
    "def getPrediction(firstpredictions,preds,k1,k2):\n",
    "    \n",
    "    results=[]\n",
    "    \n",
    "    for child in firstpredictions:\n",
    "\n",
    "        childrens=[]\n",
    "        getHierIndexs(child, preds, k1, childrens, rec=1)\n",
    "        \n",
    "        results.append(child)\n",
    "        results.extend(childrens)\n",
    "\n",
    "        for grandchild in childrens:\n",
    "            grandchildren=[]\n",
    "            getHierIndexs(grandchild,preds,k2,grandchildren)\n",
    "            results.extend(grandchildren)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "def top_k_accuracy(preds, true_labels, k0=1,k1=1,k2=1):\n",
    "    n0_level=levels[0]\n",
    "    n0_level_preds=preds[:,n0_level]\n",
    "    index_map=dict(zip(range(len(n0_level)),n0_level))\n",
    "    \n",
    "    k0=min(len(n0_level),k0)\n",
    "    top_k0_indexs= np.argpartition(n0_level_preds, -k0)[:,-k0:]\n",
    "    org_k0_indexs= [[index_map[j] for j in i] for i in top_k0_indexs]\n",
    "    \n",
    "    #preds_k=Parallel(n_jobs=num_processors)(delayed(getPrediction)(org_k0_indexs[i],preds[i,:],k1,k2) for i in range(len(org_k0_indexs)))    \n",
    "    preds_k=[getPrediction(org_k0_indexs[i],preds[i,:],k1,k2) for i in range(len(org_k0_indexs))]    \n",
    "    clusters=[np.where(t_label == 1)[0] for t_label in true_labels]\n",
    "    \n",
    "    corrects=[bool(set(clusters[i]) & set(preds_k[i])) for i in range(len(preds_k))]\n",
    "    \n",
    "    return sum(corrects)/len(corrects)\n",
    "\n",
    "# preds=np.array([\n",
    "#     [0.1,0.7,0.4,0.6],\n",
    "#     [0,1,0,0],\n",
    "#     [0.3,0.1,0,0.4]])\n",
    "# labels=np.array([\n",
    "#     [1,0,0,0],\n",
    "#     [0,1,0,0],\n",
    "#     [0,0,0,1]])\n",
    "\n",
    "# print(top_k_accuracy(preds,labels,k0=1,k1=1,k2=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_links(batch):    \n",
    "    POS=batch['pos']\n",
    "    NEG=batch['neg']\n",
    "    POS_label=batch['pos_label']\n",
    "    NEG_label=batch['neg_label']\n",
    "    CWE_index=torch.cat((POS.view(-1),NEG.view(-1)))    \n",
    "    CVE_index=torch.cat((torch.tensor(np.repeat(range(POS.shape[0]),K_NEG_LINKS),dtype=torch.long),\n",
    "               torch.tensor(np.repeat(range(NEG.shape[0]),K_NEG_LINKS),dtype=torch.long)))\n",
    "    \n",
    "    true_links=torch.cat((POS_label.view(-1),NEG_label.view(-1)))\n",
    "    \n",
    "    return CVE_index, CWE_index, true_links\n",
    "\n",
    "def prepare_all_links(nCVEs=2,nCWEs=3):    \n",
    "    CVE_index=torch.tensor(np.repeat(range(nCVEs),nCWEs),dtype=torch.long)\n",
    "    CWE_index=torch.tensor(np.tile(range(nCWEs), nCVEs),dtype=torch.long)    \n",
    "    return CVE_index, CWE_index\n",
    "\n",
    "# prepare_all_links()\n",
    "# iter_train_dataloader=iter(dataProcessor.train_dataloader())\n",
    "# batch=next(iter_train_dataloader)\n",
    "# print(batch)\n",
    "# prepare_links(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LINK_evaluate_model(dataloader, c_dataloader):\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    eval_accuracies=np.zeros(len(TOP_K0))\n",
    "    \n",
    "    num_batches=len(dataloader)\n",
    "    total_loss = 0\n",
    "    loss_value = -1\n",
    "    nb_steps=0\n",
    "    nb_links=0\n",
    "    step_time=0\n",
    "    total_step_time=0\n",
    "    current_time=0\n",
    "    epoch_start=time.time()\n",
    "    total_acc=0\n",
    "    step_acc=0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        nCWEs=class_batch['o_labels'].shape[0]\n",
    "        \n",
    "        for key,value in class_batch.items(): \n",
    "            class_batch[key]=value.to(device)\n",
    "        \n",
    "        (_, CWE_pooled)=model.base_model(class_batch) #0-classlmloss, 1-classpooled \n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s'.format(\n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start)\n",
    "                )\n",
    "            \n",
    "            nCVEs=batch['o_labels'].shape[0]            \n",
    "            batch['nCVEs'], batch['nCWEs']=prepare_all_links(nCVEs,nCWEs)            \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "            \n",
    "            (_, CVE_pooled) = model.base_model(batch)\n",
    "            \n",
    "            CVE_vectors=CVE_pooled[batch['nCVEs']]\n",
    "            CWE_vectors=CWE_pooled[batch['nCWEs']]\n",
    "            \n",
    "            (_, logits)=model.link_model(CVE_vectors,CWE_vectors)\n",
    "            \n",
    "            logits=(torch.nn.functional.softmax(logits,dim=1))[:,1]\n",
    "            logits=logits.view(nCVEs,-1)\n",
    "            loggits=logits.detach().cpu().numpy()\n",
    "            true_labels=batch['o_labels'].cpu().numpy()\n",
    "            \n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "            \n",
    "            for k_i in range(len(TOP_K0)):\n",
    "                tmp_eval=top_k_accuracy(loggits, true_labels, TOP_K0[k_i], TOP_K1[k_i], TOP_K2[k_i])*len(logits)\n",
    "                eval_accuracies[k_i]+=tmp_eval\n",
    "                \n",
    "            nb_eval_examples+=len(logits)            \n",
    "            \n",
    "    eval_accuracies=eval_accuracies/nb_eval_examples\n",
    "        \n",
    "    print(\"-\"*25)\n",
    "    for k_i in range(len(TOP_K0)):\n",
    "        print(\" Top {0},{1},{2}... Accuracy: {3:.4f}\".format(TOP_K0[k_i],TOP_K1[k_i],TOP_K2[k_i], eval_accuracies[k_i]))\n",
    "    print(\"-\"*25)\n",
    "    \n",
    "    return eval_accuracies\n",
    "\n",
    "#LINK_evaluate_model(val_dataloaderNC, class_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_links(dataloader, c_dataloader):\n",
    "    num_batches=len(dataloader)\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_value = -1\n",
    "    nb_steps=0\n",
    "    nb_links=0\n",
    "    step_time=0\n",
    "    total_step_time=0\n",
    "    current_time=0\n",
    "    epoch_start=time.time()\n",
    "    total_acc=0\n",
    "    step_acc=0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        for key,value in class_batch.items(): \n",
    "            class_batch[key]=value.to(device)\n",
    "        \n",
    "        class_outputs=model.base_model(class_batch) #0-classlmloss, 1-classpooled\n",
    "        class_lm_loss=class_outputs[0]\n",
    "        \n",
    "        if class_lm_loss is not None:\n",
    "            total_loss+=(args.lm_lambda)*(class_lm_loss.mean()).item()\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s, loss_step {:0.4f}, loss_epoch {:0.4f} - eval_f1_step {:0.4f}, eval_f1_epoch {:0.4f}'.format(\n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start, \n",
    "                    loss_value,\n",
    "                    total_loss/max(nb_steps,1),\n",
    "                    step_acc,\n",
    "                    total_acc/max(nb_links,1)))\n",
    "\n",
    "            batch['CVE_index'], batch['CWE_index'], batch['true_labels']=prepare_links(batch)       \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "            \n",
    "            outputs = model(batch, class_outputs[1]) #0-loss, 1-logits, 2-true-links            \n",
    "            \n",
    "            loss = outputs[0].mean()\n",
    "\n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=loss_value\n",
    "            logits=(torch.nn.functional.softmax(outputs[1].detach(),dim=1))\n",
    "            logits= logits.cpu().numpy()\n",
    "            true_links=outputs[2].detach().cpu().numpy()\n",
    "\n",
    "            nb_steps+=1\n",
    "            nb_links+=len(true_links)\n",
    "\n",
    "            step_acc=link_f1_score(logits,true_links)\n",
    "            total_acc+=step_acc*len(true_links)\n",
    "    \n",
    "    eval_loss=total_loss/nb_steps\n",
    "    eval_accuracy=total_acc/nb_links\n",
    "    \n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "#Evaluate_links(val_dataloaderNC, class_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0181 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 0/10 Batch 100/333 - 0.7659 s/it, 76.5408 s - Elapsed: 79.0126 s, loss_step 0.4221, loss_epoch 0.5300 - train_f1_step 0.8147, train_f1_epoch 0.7373\n",
      "Epoch 0/10 Batch 200/333 - 0.7548 s/it, 152.7958 s - Elapsed: 157.5972 s, loss_step 0.3365, loss_epoch 0.4738 - train_f1_step 0.8474, train_f1_epoch 0.7734\n",
      "Epoch 0/10 Batch 300/333 - 0.7642 s/it, 229.2448 s - Elapsed: 236.4837 s, loss_step 0.2957, loss_epoch 0.4185 - train_f1_step 0.8722, train_f1_epoch 0.8056\n",
      "Train loss: 0.4030\n",
      "Train F1-Score: 0.8143\n",
      "Train time: 255.6160 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0534 s\n",
      "Batch 100/333 - 0.1200 s/it, 11.4341 s - Elapsed: 12.8095 s\n",
      "Batch 200/333 - 0.1130 s/it, 22.7562 s - Elapsed: 25.4529 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.0765 s - Elapsed: 38.0908 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.4402\n",
      " Top 3,2,1... Accuracy: 0.6754\n",
      " Top 5,2,2... Accuracy: 0.8132\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0402 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2832\n",
      " Eval F1-Score: 0.8840\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0387 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.4450\n",
      " Top 3,2,1... Accuracy: 0.6363\n",
      " Top 5,2,2... Accuracy: 0.7757\n",
      "-------------------------\n",
      "Saving model....acc: 0.44501655020228026\n",
      "Epoch 1/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0263 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 1/10 Batch 100/333 - 0.7647 s/it, 76.5167 s - Elapsed: 79.0229 s, loss_step 0.1859, loss_epoch 0.2333 - train_f1_step 0.9199, train_f1_epoch 0.9067\n",
      "Epoch 1/10 Batch 200/333 - 0.7635 s/it, 153.2027 s - Elapsed: 158.1548 s, loss_step 0.1981, loss_epoch 0.2262 - train_f1_step 0.9398, train_f1_epoch 0.9099\n",
      "Epoch 1/10 Batch 300/333 - 0.7684 s/it, 229.7501 s - Elapsed: 237.1732 s, loss_step 0.1406, loss_epoch 0.2183 - train_f1_step 0.9499, train_f1_epoch 0.9133\n",
      "Train loss: 0.2161\n",
      "Train F1-Score: 0.9142\n",
      "Train time: 255.3699 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0540 s\n",
      "Batch 100/333 - 0.1131 s/it, 11.4252 s - Elapsed: 12.8008 s\n",
      "Batch 200/333 - 0.1131 s/it, 22.7298 s - Elapsed: 25.3743 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.0313 s - Elapsed: 37.9133 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.5889\n",
      " Top 3,2,1... Accuracy: 0.8246\n",
      " Top 5,2,2... Accuracy: 0.8984\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0369 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2331\n",
      " Eval F1-Score: 0.9115\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0340 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.5579\n",
      " Top 3,2,1... Accuracy: 0.7863\n",
      " Top 5,2,2... Accuracy: 0.8713\n",
      "-------------------------\n",
      "Saving model....acc: 0.5579257079808754\n",
      "Epoch 2/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0335 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 2/10 Batch 100/333 - 0.7661 s/it, 76.8104 s - Elapsed: 79.3343 s, loss_step 0.2549, loss_epoch 0.1695 - train_f1_step 0.8987, train_f1_epoch 0.9376\n",
      "Epoch 2/10 Batch 200/333 - 0.7639 s/it, 153.3503 s - Elapsed: 158.3964 s, loss_step 0.1729, loss_epoch 0.1647 - train_f1_step 0.9277, train_f1_epoch 0.9372\n",
      "Epoch 2/10 Batch 300/333 - 0.7661 s/it, 229.8820 s - Elapsed: 237.4451 s, loss_step 0.1271, loss_epoch 0.1630 - train_f1_step 0.9549, train_f1_epoch 0.9383\n",
      "Train loss: 0.1627\n",
      "Train F1-Score: 0.9384\n",
      "Train time: 255.1930 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0661 s\n",
      "Batch 100/333 - 0.1130 s/it, 11.4052 s - Elapsed: 12.6990 s\n",
      "Batch 200/333 - 0.1130 s/it, 22.7101 s - Elapsed: 25.2597 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.3574 s - Elapsed: 38.2702 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6997\n",
      " Top 3,2,1... Accuracy: 0.8800\n",
      " Top 5,2,2... Accuracy: 0.9331\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0382 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2084\n",
      " Eval F1-Score: 0.9188\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0388 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6462\n",
      " Top 3,2,1... Accuracy: 0.8264\n",
      " Top 5,2,2... Accuracy: 0.8941\n",
      "-------------------------\n",
      "Saving model....acc: 0.6461934534755425\n",
      "Epoch 3/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0198 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 3/10 Batch 100/333 - 0.7639 s/it, 76.5920 s - Elapsed: 79.0778 s, loss_step 0.1121, loss_epoch 0.1371 - train_f1_step 0.9508, train_f1_epoch 0.9487\n",
      "Epoch 3/10 Batch 200/333 - 0.7665 s/it, 153.1188 s - Elapsed: 158.0698 s, loss_step 0.2898, loss_epoch 0.1328 - train_f1_step 0.9181, train_f1_epoch 0.9502\n",
      "Epoch 3/10 Batch 300/333 - 0.7652 s/it, 229.7574 s - Elapsed: 237.1562 s, loss_step 0.1028, loss_epoch 0.1297 - train_f1_step 0.9732, train_f1_epoch 0.9514\n",
      "Train loss: 0.1295\n",
      "Train F1-Score: 0.9514\n",
      "Train time: 255.0290 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0571 s\n",
      "Batch 100/333 - 0.1134 s/it, 11.4176 s - Elapsed: 12.7408 s\n",
      "Batch 200/333 - 0.1130 s/it, 22.7355 s - Elapsed: 25.3938 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.0450 s - Elapsed: 38.0004 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7496\n",
      " Top 3,2,1... Accuracy: 0.9066\n",
      " Top 5,2,2... Accuracy: 0.9588\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0373 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2103\n",
      " Eval F1-Score: 0.9221\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0407 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7135\n",
      " Top 3,2,1... Accuracy: 0.8558\n",
      " Top 5,2,2... Accuracy: 0.9044\n",
      "-------------------------\n",
      "Saving model....acc: 0.7134976094152262\n",
      "Epoch 4/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0204 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 4/10 Batch 100/333 - 0.7649 s/it, 76.8459 s - Elapsed: 79.3257 s, loss_step 0.1575, loss_epoch 0.1071 - train_f1_step 0.9495, train_f1_epoch 0.9611\n",
      "Epoch 4/10 Batch 200/333 - 0.7664 s/it, 153.5275 s - Elapsed: 158.4576 s, loss_step 0.1051, loss_epoch 0.1052 - train_f1_step 0.9518, train_f1_epoch 0.9609\n",
      "Epoch 4/10 Batch 300/333 - 0.7598 s/it, 229.6759 s - Elapsed: 236.9245 s, loss_step 0.0647, loss_epoch 0.1046 - train_f1_step 0.9797, train_f1_epoch 0.9611\n",
      "Train loss: 0.1057\n",
      "Train F1-Score: 0.9607\n",
      "Train time: 254.9302 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0565 s\n",
      "Batch 100/333 - 0.1133 s/it, 11.4485 s - Elapsed: 12.8285 s\n",
      "Batch 200/333 - 0.1130 s/it, 22.7672 s - Elapsed: 25.4636 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.0749 s - Elapsed: 38.0255 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7738\n",
      " Top 3,2,1... Accuracy: 0.9265\n",
      " Top 5,2,2... Accuracy: 0.9709\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0430 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.1968\n",
      " Eval F1-Score: 0.9314\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0378 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7172\n",
      " Top 3,2,1... Accuracy: 0.8466\n",
      " Top 5,2,2... Accuracy: 0.9062\n",
      "-------------------------\n",
      "Saving model....acc: 0.7171754321441707\n",
      "Epoch 5/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0206 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 5/10 Batch 100/333 - 0.7634 s/it, 76.7185 s - Elapsed: 79.1689 s, loss_step 0.0756, loss_epoch 0.0925 - train_f1_step 0.9775, train_f1_epoch 0.9668\n",
      "Epoch 5/10 Batch 200/333 - 0.7633 s/it, 153.2494 s - Elapsed: 158.1480 s, loss_step 0.0637, loss_epoch 0.0890 - train_f1_step 0.9815, train_f1_epoch 0.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Batch 300/333 - 0.7614 s/it, 229.7328 s - Elapsed: 237.0767 s, loss_step 0.1654, loss_epoch 0.0883 - train_f1_step 0.9403, train_f1_epoch 0.9682\n",
      "Train loss: 0.0891\n",
      "Train F1-Score: 0.9679\n",
      "Train time: 254.9929 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0569 s\n",
      "Batch 100/333 - 0.1132 s/it, 11.7230 s - Elapsed: 13.0302 s\n",
      "Batch 200/333 - 0.1128 s/it, 23.0362 s - Elapsed: 25.6427 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.3426 s - Elapsed: 38.2093 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7953\n",
      " Top 3,2,1... Accuracy: 0.9420\n",
      " Top 5,2,2... Accuracy: 0.9782\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0429 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2069\n",
      " Eval F1-Score: 0.9305\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0325 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7168\n",
      " Top 3,2,1... Accuracy: 0.8533\n",
      " Top 5,2,2... Accuracy: 0.9055\n",
      "-------------------------\n",
      "Epoch 6/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0209 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 6/10 Batch 100/333 - 1.0081 s/it, 76.7327 s - Elapsed: 79.1948 s, loss_step 0.0856, loss_epoch 0.0751 - train_f1_step 0.9724, train_f1_epoch 0.9738\n",
      "Epoch 6/10 Batch 200/333 - 0.7612 s/it, 153.2734 s - Elapsed: 158.1648 s, loss_step 0.0698, loss_epoch 0.0773 - train_f1_step 0.9777, train_f1_epoch 0.9728\n",
      "Epoch 6/10 Batch 300/333 - 0.7702 s/it, 229.8041 s - Elapsed: 237.1821 s, loss_step 0.0844, loss_epoch 0.0782 - train_f1_step 0.9681, train_f1_epoch 0.9727\n",
      "Train loss: 0.0779\n",
      "Train F1-Score: 0.9729\n",
      "Train time: 255.0573 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0580 s\n",
      "Batch 100/333 - 0.1129 s/it, 11.4360 s - Elapsed: 12.8181 s\n",
      "Batch 200/333 - 0.1130 s/it, 22.7385 s - Elapsed: 25.3735 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.0410 s - Elapsed: 37.9334 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8196\n",
      " Top 3,2,1... Accuracy: 0.9529\n",
      " Top 5,2,2... Accuracy: 0.9837\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0421 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2141\n",
      " Eval F1-Score: 0.9313\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0354 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7300\n",
      " Top 3,2,1... Accuracy: 0.8628\n",
      " Top 5,2,2... Accuracy: 0.9095\n",
      "-------------------------\n",
      "Saving model....acc: 0.7300478116954763\n",
      "Epoch 7/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0274 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 7/10 Batch 100/333 - 0.7617 s/it, 76.4617 s - Elapsed: 78.9125 s, loss_step 0.0445, loss_epoch 0.0668 - train_f1_step 0.9883, train_f1_epoch 0.9773\n",
      "Epoch 7/10 Batch 200/333 - 0.7647 s/it, 152.9847 s - Elapsed: 157.9200 s, loss_step 0.0902, loss_epoch 0.0694 - train_f1_step 0.9546, train_f1_epoch 0.9761\n",
      "Epoch 7/10 Batch 300/333 - 0.7650 s/it, 229.6460 s - Elapsed: 237.0336 s, loss_step 0.0624, loss_epoch 0.0695 - train_f1_step 0.9747, train_f1_epoch 0.9756\n",
      "Train loss: 0.0694\n",
      "Train F1-Score: 0.9759\n",
      "Train time: 254.8745 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0555 s\n",
      "Batch 100/333 - 0.1134 s/it, 11.4136 s - Elapsed: 12.7310 s\n",
      "Batch 200/333 - 0.1129 s/it, 22.7207 s - Elapsed: 25.2949 s\n",
      "Batch 300/333 - 0.1129 s/it, 34.0193 s - Elapsed: 37.8036 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8326\n",
      " Top 3,2,1... Accuracy: 0.9596\n",
      " Top 5,2,2... Accuracy: 0.9880\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0726 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2151\n",
      " Eval F1-Score: 0.9340\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0418 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7363\n",
      " Top 3,2,1... Accuracy: 0.8613\n",
      " Top 5,2,2... Accuracy: 0.9077\n",
      "-------------------------\n",
      "Saving model....acc: 0.7363001103346819\n",
      "Epoch 8/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0232 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 8/10 Batch 100/333 - 0.7664 s/it, 76.7970 s - Elapsed: 79.2676 s, loss_step 0.0605, loss_epoch 0.0648 - train_f1_step 0.9775, train_f1_epoch 0.9767\n",
      "Epoch 8/10 Batch 200/333 - 0.7623 s/it, 152.9048 s - Elapsed: 157.6178 s, loss_step 0.0317, loss_epoch 0.0632 - train_f1_step 0.9904, train_f1_epoch 0.9780\n",
      "Epoch 8/10 Batch 300/333 - 0.7651 s/it, 229.6085 s - Elapsed: 236.7611 s, loss_step 0.1012, loss_epoch 0.0637 - train_f1_step 0.9689, train_f1_epoch 0.9777\n",
      "Train loss: 0.0633\n",
      "Train F1-Score: 0.9778\n",
      "Train time: 254.8581 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0555 s\n",
      "Batch 100/333 - 0.1130 s/it, 11.4165 s - Elapsed: 12.7223 s\n",
      "Batch 200/333 - 0.1130 s/it, 22.7214 s - Elapsed: 25.2804 s\n",
      "Batch 300/333 - 0.1135 s/it, 34.0362 s - Elapsed: 37.8767 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8387\n",
      " Top 3,2,1... Accuracy: 0.9643\n",
      " Top 5,2,2... Accuracy: 0.9913\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0452 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2151\n",
      " Eval F1-Score: 0.9341\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0374 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7426\n",
      " Top 3,2,1... Accuracy: 0.8680\n",
      " Top 5,2,2... Accuracy: 0.9092\n",
      "-------------------------\n",
      "Saving model....acc: 0.7425524089738874\n",
      "Epoch 9/10 Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0203 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Epoch 9/10 Batch 100/333 - 0.7677 s/it, 76.8821 s - Elapsed: 79.3573 s, loss_step 0.0613, loss_epoch 0.0588 - train_f1_step 0.9747, train_f1_epoch 0.9810\n",
      "Epoch 9/10 Batch 200/333 - 0.7651 s/it, 153.3982 s - Elapsed: 158.3575 s, loss_step 0.0746, loss_epoch 0.0589 - train_f1_step 0.9680, train_f1_epoch 0.9808\n",
      "Epoch 9/10 Batch 300/333 - 0.7649 s/it, 229.9209 s - Elapsed: 237.3992 s, loss_step 0.0789, loss_epoch 0.0597 - train_f1_step 0.9437, train_f1_epoch 0.9803\n",
      "Train loss: 0.0595\n",
      "Train F1-Score: 0.9802\n",
      "Train time: 255.1643 sec\n",
      "Evaluate train model\n",
      "Batch 0/333 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0553 s\n",
      "Batch 100/333 - 0.1133 s/it, 11.8142 s - Elapsed: 13.2048 s\n",
      "Batch 200/333 - 0.1129 s/it, 23.1392 s - Elapsed: 25.8311 s\n",
      "Batch 300/333 - 0.1130 s/it, 34.4454 s - Elapsed: 38.4024 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8427\n",
      " Top 3,2,1... Accuracy: 0.9646\n",
      " Top 5,2,2... Accuracy: 0.9921\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0392 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.2158\n",
      " Eval F1-Score: 0.9351\n",
      "Evaluate validation model\n",
      "Batch 0/85 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0383 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7403\n",
      " Top 3,2,1... Accuracy: 0.8680\n",
      " Top 5,2,2... Accuracy: 0.9095\n",
      "-------------------------\n",
      "Link Prediction Training complete!\n",
      "Saving Last model\n",
      "Batch 0/524 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0399 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      "Batch 100/524 - 0.0271 s/it, 2.3707 s - Elapsed: 12.4841 s, loss_step 0.3448, loss_epoch 0.1893 - eval_f1_step 0.9022, eval_f1_epoch 0.9454\n",
      "Batch 200/524 - 0.0232 s/it, 4.6343 s - Elapsed: 24.8311 s, loss_step 0.1789, loss_epoch 0.2073 - eval_f1_step 0.9338, eval_f1_epoch 0.9392\n",
      "Batch 300/524 - 0.0224 s/it, 6.9147 s - Elapsed: 37.1913 s, loss_step 0.1133, loss_epoch 0.2075 - eval_f1_step 0.9557, eval_f1_epoch 0.9392\n",
      "Batch 400/524 - 0.0219 s/it, 9.1785 s - Elapsed: 49.5184 s, loss_step 0.0887, loss_epoch 0.2210 - eval_f1_step 0.9719, eval_f1_epoch 0.9359\n",
      "Batch 500/524 - 0.0216 s/it, 11.4710 s - Elapsed: 61.8431 s, loss_step 0.0951, loss_epoch 0.2221 - eval_f1_step 0.9704, eval_f1_epoch 0.9359\n",
      " Average test loss: 0.2180\n",
      "Evaluate test model\n",
      "Batch 0/524 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0415 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/524 - 0.1095 s/it, 11.1166 s - Elapsed: 12.5225 s\n",
      "Batch 200/524 - 0.1094 s/it, 22.0861 s - Elapsed: 24.7498 s\n",
      "Batch 300/524 - 0.1095 s/it, 33.3471 s - Elapsed: 37.3289 s\n",
      "Batch 400/524 - 0.1095 s/it, 44.3234 s - Elapsed: 49.6589 s\n",
      "Batch 500/524 - 0.1095 s/it, 55.2894 s - Elapsed: 61.8952 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7261\n",
      " Top 3,2,1... Accuracy: 0.8508\n",
      " Top 5,2,2... Accuracy: 0.9003\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_model(dataloader, c_dataloader, v_dataloader=None, t_dataloader=None):    \n",
    "    optimizer, scheduler = model.configure_optimizers()\n",
    "    num_batches=len(dataloader)\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_accs=[]\n",
    "    val_losses=[]\n",
    "    val_accs=[]\n",
    "    train_class_acc=[]\n",
    "    val_class_acc=[]\n",
    "    best_val=-1\n",
    "    \n",
    "    total_train_time=0\n",
    "    \n",
    "    for epoch_i in range(args.epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        loss_value = -1\n",
    "        nb_steps=0\n",
    "        nb_links=0\n",
    "        step_time=0\n",
    "        total_step_time=0\n",
    "        current_time=0\n",
    "        epoch_start=time.time()\n",
    "        total_acc=0\n",
    "        step_acc=0\n",
    "        \n",
    "        model.train()\n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        for key,value in class_batch.items(): class_batch[key]=value.to(device)\n",
    "            \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Epoch {:}/{:} Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s, loss_step {:0.4f}, loss_epoch {:0.4f} - train_f1_step {:0.4f}, train_f1_epoch {:0.4f}'.format(\n",
    "                    epoch_i,\n",
    "                    args.epochs, \n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start, \n",
    "                    loss_value,\n",
    "                    total_loss/max(nb_steps,1),\n",
    "                    step_acc,\n",
    "                    total_acc/max(nb_links,1)))\n",
    "            \n",
    "            batch['CVE_index'], batch['CWE_index'], batch['true_labels']=prepare_links(batch)       \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "                            \n",
    "            model.zero_grad()\n",
    "            class_outputs=model.base_model(class_batch) #0-classlmloss, 1-classpooled\n",
    "            class_lm_loss=class_outputs[0]  \n",
    "            \n",
    "            outputs = model(batch, class_outputs[1]) #0-loss, 1-logits, 2-true-links        \n",
    "            \n",
    "            loss = outputs[0].mean()            \n",
    "            if class_lm_loss is not None:\n",
    "                loss+= (args.lm_lambda)*class_lm_loss.mean()\n",
    "            \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "            \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "            \n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "            \n",
    "            #stats\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=loss_value\n",
    "            logits=(torch.nn.functional.softmax(outputs[1].detach(),dim=1))\n",
    "            logits= logits.cpu().numpy()\n",
    "            true_links=outputs[2].detach().cpu().numpy()\n",
    "            \n",
    "            nb_steps+=1\n",
    "            nb_links+=len(true_links)\n",
    "            \n",
    "            step_acc=link_f1_score(logits,true_links)\n",
    "            total_acc+=step_acc*len(true_links)\n",
    "            \n",
    "        total_train_time+=total_step_time\n",
    "\n",
    "        avg_train_loss=total_loss/nb_steps\n",
    "        avg_train_acc=total_acc/nb_links\n",
    "        \n",
    "        print(\"Train loss: {0:.4f}\".format(avg_train_loss))\n",
    "        print(\"Train F1-Score: {0:.4f}\".format(avg_train_acc))\n",
    "        print(\"Train time: {0:.4f} sec\".format(total_step_time))\n",
    "        \n",
    "        if args.performance_mode=='True':return\n",
    "    \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accs.append(avg_train_acc)\n",
    "        \n",
    "        print(\"Evaluate train model\")\n",
    "        train_cwe_acc=LINK_evaluate_model(dataloader,c_dataloader)\n",
    "        train_class_acc.append(train_cwe_acc[0])\n",
    "        #print(train_cwe_acc)\n",
    "        \n",
    "        if v_dataloader is not None:\n",
    "            print(\"Validation.....\")        \n",
    "            eval_loss, eval_accuracy=Evaluate_links(v_dataloader, c_dataloader)      \n",
    "        \n",
    "            print(\" Average evaluation loss: {0:.4f}\".format(eval_loss))\n",
    "            print(\" Eval F1-Score: {0:.4f}\".format(eval_accuracy))\n",
    "        \n",
    "            val_accs.append(eval_accuracy)\n",
    "            val_losses.append(eval_loss)\n",
    "         \n",
    "            print(\"Evaluate validation model\")\n",
    "            val_cwe_acc=LINK_evaluate_model(v_dataloader,c_dataloader)\n",
    "            val_class_acc.append(val_cwe_acc[0])\n",
    "            \n",
    "            if args.checkpointing=='True':\n",
    "                if(best_val is None or val_cwe_acc[0]>best_val):\n",
    "                    best_val=val_cwe_acc[0]\n",
    "                    print(\"Saving model....acc:\",best_val)\n",
    "                    torch.save(model.state_dict(), args.MODEL_DIR_FILE+'_BEST')\n",
    "\n",
    "    print(\"Link Prediction Training complete!\")\n",
    "    \n",
    "    if args.checkpointing=='True':\n",
    "        print(\"Saving Last model\")\n",
    "        torch.save(model.state_dict(), args.MODEL_DIR_FILE+'_LAST')\n",
    "    \n",
    "    if t_dataloader is not None:    \n",
    "        eval_loss, eval_accuracy=Evaluate_links(t_dataloader,c_dataloader)              \n",
    "        print(\" Average test loss: {0:.4f}\".format(eval_loss))\n",
    "\n",
    "        print(\"Evaluate test model\")\n",
    "        test_cwe_acc=LINK_evaluate_model(t_dataloader,c_dataloader)        \n",
    "        log_results(args.MODEL_DIR_FILE+'_log.txt', {'test_accuracies':test_cwe_acc})\n",
    "    \n",
    "    from ipynb.fs.full.lib.Utils import save_plot\n",
    "    save_plot(train_accs, val_accs, name=args.MODEL_DIR_FILE+'_link_acc',yname='Accuracy')\n",
    "    save_plot(train_losses, val_losses, name=args.MODEL_DIR_FILE+'_link_loss',yname='Loss')\n",
    "    save_plot(train_class_acc, val_class_acc, name=args.MODEL_DIR_FILE+'_class_acc',yname='Accuracy')\n",
    "    \n",
    "    log_results(args.MODEL_DIR_FILE+'_log.txt',\n",
    "               {'train_accs':train_accs, 'val_accs':val_accs,\n",
    "                'train_losses':train_losses, 'val_losses':val_losses,\n",
    "                'train_class_acc':train_class_acc, 'val_class_acc':val_class_acc})\n",
    "    \n",
    "    log_results(args.MODEL_DIR_FILE+'_log.txt', {\n",
    "        'total_step_time':[total_step_time],\n",
    "        'num_batches':[num_batches],\n",
    "        'epochs':[args.epochs],\n",
    "        'batch_size':[args.batch_size]})\n",
    "    \n",
    "    return\n",
    "\n",
    "if args.use_rd == 'True':\n",
    "    train_model(train_dataloader, class_dataloader, val_dataloaderNC, test_dataloaderNC)\n",
    "else:\n",
    "    train_model(train_dataloaderNC, class_dataloaderNC, val_dataloaderNC, test_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Saved Model:  ./Results/Model/V2WBERT-LINK-distilbert-base-uncased-dp_LAST\n",
      "Original weights: \n",
      "link_model.module.lc_2.bias tensor([-0.0204,  0.0218], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Loaded weights: \n",
      "link_model.module.lc_2.bias tensor([-0.0204,  0.0218], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Batch 0/524 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0419 s\n",
      "Batch 100/524 - 0.1095 s/it, 11.0753 s - Elapsed: 12.3762 s\n",
      "Batch 200/524 - 0.1095 s/it, 22.0278 s - Elapsed: 24.5799 s\n",
      "Batch 300/524 - 0.1094 s/it, 32.9785 s - Elapsed: 36.7458 s\n",
      "Batch 400/524 - 0.1098 s/it, 43.9396 s - Elapsed: 48.9266 s\n",
      "Batch 500/524 - 0.1095 s/it, 54.9273 s - Elapsed: 61.2530 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7261\n",
      " Top 3,2,1... Accuracy: 0.8508\n",
      " Top 5,2,2... Accuracy: 0.9003\n",
      "-------------------------\n",
      "Batch 0/524 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0375 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      "Batch 100/524 - 0.0227 s/it, 2.3631 s - Elapsed: 12.4710 s, loss_step 0.3453, loss_epoch 0.1895 - eval_f1_step 0.9020, eval_f1_epoch 0.9453\n",
      "Batch 200/524 - 0.0220 s/it, 4.6245 s - Elapsed: 24.8121 s, loss_step 0.1767, loss_epoch 0.2074 - eval_f1_step 0.9341, eval_f1_epoch 0.9391\n",
      "Batch 300/524 - 0.0221 s/it, 6.8638 s - Elapsed: 37.1161 s, loss_step 0.1141, loss_epoch 0.2075 - eval_f1_step 0.9557, eval_f1_epoch 0.9391\n",
      "Batch 400/524 - 0.0294 s/it, 9.4173 s - Elapsed: 49.6994 s, loss_step 0.0903, loss_epoch 0.2211 - eval_f1_step 0.9717, eval_f1_epoch 0.9359\n",
      "Batch 500/524 - 0.0233 s/it, 11.6958 s - Elapsed: 61.9833 s, loss_step 0.0957, loss_epoch 0.2221 - eval_f1_step 0.9703, eval_f1_epoch 0.9359\n",
      "Test F-1 Score:  0.9367565229351711\n",
      "Test Complete......\n"
     ]
    }
   ],
   "source": [
    "def test_model(t_dataloader, c_dataloader):\n",
    "    MODEL_NAME=args.MODEL_DIR_FILE+'_LAST'    \n",
    "\n",
    "    if os.path.exists(MODEL_NAME): \n",
    "        print('Loading Saved Model: ',MODEL_NAME)        \n",
    "    else: \n",
    "        print(\"File not found: \",MODEL_NAME)\n",
    "        return\n",
    "    \n",
    "    print(\"Original weights: \");print_model_value(model)\n",
    "    checkpoint = torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    print(\"Loaded weights: \");print_model_value(model)    \n",
    "    LINK_evaluate_model(test_dataloaderNC, class_dataloaderNC)    \n",
    "    \n",
    "    eval_loss, eval_accuracy = Evaluate_links(test_dataloaderNC, class_dataloaderNC)\n",
    "    print(\"Test F-1 Score: \", eval_accuracy)\n",
    "    \n",
    "    print(\"Test Complete......\")\n",
    "    \n",
    "    return\n",
    "\n",
    "if args.checkpointing=='True':\n",
    "    test_model(test_dataloaderNC, class_dataloaderNC)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
