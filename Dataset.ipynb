{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import logging\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def size_repr(key, item, indent=0):\n",
    "    indent_str = ' ' * indent\n",
    "    if torch.is_tensor(item) and item.dim() == 0:\n",
    "        out = item.item()\n",
    "    elif torch.is_tensor(item):\n",
    "        out = str(list(item.size()))\n",
    "#     elif isinstance(item, SparseTensor):\n",
    "#         out = str(item.sizes())[:-1] + f', nnz={item.nnz()}]'\n",
    "    elif isinstance(item, list) or isinstance(item, tuple):\n",
    "        out = str([len(item)])\n",
    "    elif isinstance(item, dict):\n",
    "        lines = [indent_str + size_repr(k, v, 2) for k, v in item.items()]\n",
    "        out = '{\\n' + ',\\n'.join(lines) + '\\n' + indent_str + '}'\n",
    "    elif isinstance(item, str):\n",
    "        out = f'\"{item}\"'\n",
    "    else:\n",
    "        out = str(item)\n",
    "\n",
    "    return f'{indent_str}{key}={out}'\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None,\n",
    "                 pos=None, normal=None, face=None, **kwargs):\n",
    "        self.x = x\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.y = y\n",
    "        self.pos = pos\n",
    "        self.normal = normal\n",
    "        self.face = face\n",
    "        for key, item in kwargs.items():\n",
    "            if key == 'num_nodes':\n",
    "                self.__num_nodes__ = item\n",
    "            else:\n",
    "                self[key] = item\n",
    "                \n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, dictionary):\n",
    "        r\"\"\"Creates a data object from a python dictionary.\"\"\"\n",
    "        data = cls()\n",
    "\n",
    "        for key, item in dictionary.items():\n",
    "            data[key] = item\n",
    "\n",
    "        if torch_geometric.is_debug_enabled():\n",
    "            data.debug()\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {key: item for key, item in self}\n",
    "\n",
    "    def to_namedtuple(self):\n",
    "        keys = self.keys\n",
    "        DataTuple = collections.namedtuple('DataTuple', keys)\n",
    "        return DataTuple(*[self[key] for key in keys])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        r\"\"\"Gets the data of the attribute :obj:`key`.\"\"\"\n",
    "        return getattr(self, key, None)\n",
    "\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"Sets the attribute :obj:`key` to :obj:`value`.\"\"\"\n",
    "        setattr(self, key, value)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        r\"\"\"Returns all names of graph attributes.\"\"\"\n",
    "        keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
    "        keys = [key for key in keys if key[:2] != '__' and key[-2:] != '__']\n",
    "        return keys\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"Returns the number of all present attributes.\"\"\"\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        r\"\"\"Returns :obj:`True`, if the attribute :obj:`key` is present in the\n",
    "        data.\"\"\"\n",
    "        return key in self.keys\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        r\"\"\"Iterates over all present attributes in the data, yielding their\n",
    "        attribute names and content.\"\"\"\n",
    "        for key in sorted(self.keys):\n",
    "            yield key, self[key]\n",
    "\n",
    "    def __call__(self, *keys):\n",
    "        r\"\"\"Iterates over all attributes :obj:`*keys` in the data, yielding\n",
    "        their attribute names and content.\n",
    "        If :obj:`*keys` is not given this method will iterative over all\n",
    "        present attributes.\"\"\"\n",
    "        for key in sorted(self.keys) if not keys else keys:\n",
    "            if key in self:\n",
    "                yield key, self[key]\n",
    "\n",
    "\n",
    "    def __cat_dim__(self, key, value):\n",
    "        r\"\"\"Returns the dimension for which :obj:`value` of attribute\n",
    "        :obj:`key` will get concatenated when creating batches.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This method is for internal use only, and should only be overridden\n",
    "            if the batch concatenation process is corrupted for a specific data\n",
    "            attribute.\n",
    "        \"\"\"\n",
    "        # Concatenate `*index*` and `*face*` attributes in the last dimension.\n",
    "        if bool(re.search('(index|face)', key)):\n",
    "            return -1\n",
    "        # By default, concatenate sparse matrices diagonally.\n",
    "        elif isinstance(value, SparseTensor):\n",
    "            return (0, 1)\n",
    "        return 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        cls = str(self.__class__.__name__)\n",
    "        has_dict = any([isinstance(item, dict) for _, item in self])\n",
    "\n",
    "        if not has_dict:\n",
    "            info = [size_repr(key, item) for key, item in self]\n",
    "            return '{}({})'.format(cls, ', '.join(info))\n",
    "        else:\n",
    "            info = [size_repr(key, item, indent=2) for key, item in self]\n",
    "            return '{}(\\n{}\\n)'.format(cls, ',\\n'.join(info))\n",
    "    \n",
    "    def __apply__(self, item, func):\n",
    "        if torch.is_tensor(item):\n",
    "            return func(item)\n",
    "        elif isinstance(item, SparseTensor):\n",
    "            # Not all apply methods are supported for `SparseTensor`, e.g.,\n",
    "            # `contiguous()`. We can get around it by capturing the exception.\n",
    "            try:\n",
    "                return func(item)\n",
    "            except AttributeError:\n",
    "                return item\n",
    "        elif isinstance(item, (tuple, list)):\n",
    "            return [self.__apply__(v, func) for v in item]\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: self.__apply__(v, func) for k, v in item.items()}\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "    def apply(self, func, *keys):\n",
    "        r\"\"\"Applies the function :obj:`func` to all tensor attributes\n",
    "        :obj:`*keys`. If :obj:`*keys` is not given, :obj:`func` is applied to\n",
    "        all present attributes.\n",
    "        \"\"\"\n",
    "        for key, item in self(*keys):\n",
    "            self[key] = self.__apply__(item, func)\n",
    "        return self\n",
    "\n",
    "    def contiguous(self, *keys):\n",
    "        r\"\"\"Ensures a contiguous memory layout for all attributes :obj:`*keys`.\n",
    "        If :obj:`*keys` is not given, all present attributes are ensured to\n",
    "        have a contiguous memory layout.\"\"\"\n",
    "        return self.apply(lambda x: x.contiguous(), *keys)\n",
    "\n",
    "\n",
    "    def to(self, device, *keys, **kwargs):\n",
    "        r\"\"\"Performs tensor dtype and/or device conversion to all attributes\n",
    "        :obj:`*keys`.\n",
    "        If :obj:`*keys` is not given, the conversion is applied to all present\n",
    "        attributes.\"\"\"\n",
    "        return self.apply(lambda x: x.to(device, **kwargs), *keys)\n",
    "\n",
    "\n",
    "    def clone(self):\n",
    "        return self.__class__.from_dict({\n",
    "            k: v.clone() if torch.is_tensor(v) else copy.deepcopy(v)\n",
    "            for k, v in self.__dict__.items()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(DIR):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    \n",
    "    data= pickle.load(open(DIR+'NVD_data', \"rb\" ))\n",
    "    df_CVE_merged=pd.read_csv(DIR+'NVD_CVE.csv',low_memory=False)\n",
    "    df_CWE=pd.read_csv(DIR+'NVD_CWE.csv',low_memory=False)\n",
    "\n",
    "    return data, df_CVE_merged, df_CWE\n",
    "\n",
    "#data, df_CVE_merged, df_CWE = getDataset('./NVD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomDataset(DIR, train_percent=0.70, validation_percent=0.10):    \n",
    "    data, df_CVE_merged, df_CWE = getDataset(DIR)\n",
    "    labels=data.y\n",
    "    NUM_CLASSES=len(data.y[0])\n",
    "    \n",
    "    data_all_mask=data.train_mask|data.val_mask|data.test_mask\n",
    "    all_mask= (data_all_mask == True).nonzero().flatten().numpy()\n",
    "    \n",
    "    CWEs_data={}\n",
    "    \n",
    "    for key in all_mask:\n",
    "        row=labels[key]\n",
    "        cwes=np.where(row == 1)[0]\n",
    "        for cwe in cwes:\n",
    "            if cwe in CWEs_data:\n",
    "                CWEs_data[cwe].append(key)\n",
    "            else:\n",
    "                CWEs_data[cwe]=[key]\n",
    "\n",
    "#     for key in range(NUM_CLASSES): \n",
    "#         print(key,end='->')\n",
    "#         if key in CWEs_data:\n",
    "#             print(len(CWEs_data[key]))\n",
    "#         else:\n",
    "#             print(0)\n",
    "            \n",
    "    test_percent=1.0-train_percent-validation_percent\n",
    "    \n",
    "    train_set=[]\n",
    "    validation_set=[]\n",
    "    test_set=[]\n",
    "    \n",
    "    for key in range(NUM_CLASSES): \n",
    "        if key in CWEs_data:\n",
    "            count=len(CWEs_data[key])\n",
    "            caselist=CWEs_data[key]\n",
    "            np.random.shuffle(caselist)            \n",
    "            \n",
    "            train_len=int(count*train_percent)\n",
    "            val_len=int(count*validation_percent)\n",
    "            \n",
    "            train_set.extend(caselist[:train_len])\n",
    "            validation_set.extend(caselist[train_len:train_len+val_len])\n",
    "            test_set.extend(caselist[train_len+val_len:])\n",
    "    \n",
    "#     print(\"Train Size: \",len(train_set))\n",
    "#     print(\"Val Size: \", len(validation_set))\n",
    "#     print(\"Test Size: \", len(test_set))\n",
    "            \n",
    "    data.train_mask[:]=False\n",
    "    data.val_mask[:]=False    \n",
    "    data.test_mask[:]=False \n",
    "    \n",
    "    data.train_mask[train_set]=True\n",
    "    data.val_mask[validation_set]=True\n",
    "    data.test_mask[test_set]=True\n",
    "    \n",
    "    return data, df_CVE_merged, df_CWE\n",
    "\n",
    "#data, df_CVE_merged, df_CWE = getRandomDataset('./Dataset/NVD/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data)\n",
    "# new_data=Data()\n",
    "# for key in data.keys:\n",
    "#     new_data[key]=data[key]\n",
    "# import pickle\n",
    "# pickle.dump(new_data,open('./NVD/NVD_data', \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDummyDataset():\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    sentences=[\n",
    "        \"Human machine interface for lab abc computer applications\",\n",
    "        \"A survey of user opinion of computer system response time\",\n",
    "        \"The EPS user interface management system\",\n",
    "        \"System anad human system engineering testing of EPS\",\n",
    "        \"Relation of user perceived response time to error measurement\",\n",
    "        \"The generation of random binary unordered trees\",\n",
    "        \"The intersection graph of paths in trees\",\n",
    "        \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "        \"Graph minors A survey\",\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"it's been raining cats and dogs\",\n",
    "        \"I don't know what to think any more\",\n",
    "        \"I love animals\"]\n",
    "\n",
    "    class_sentences=[\n",
    "        \"Human machine interaction of computer\",\n",
    "        \"Graph tress algorithm testing\",\n",
    "        \"EPS user are management\",\n",
    "        \"Animal dogs are animals too\"\n",
    "    ]\n",
    "\n",
    "    labels=torch.Tensor([\n",
    "        [1,0,0,0],\n",
    "        [1,0,0,0],\n",
    "        [0,0,1,0],\n",
    "        [0,0,1,0],\n",
    "        [1,0,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,0,0,1],\n",
    "        [0,0,0,1],\n",
    "        [0,0,0,1],\n",
    "        [0,0,0,1]\n",
    "    ])\n",
    "\n",
    "    class_labels=torch.Tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,0,1,0],\n",
    "        [0,0,0,1]\n",
    "    ])\n",
    "\n",
    "    train_mask=torch.tensor([1,1,1,1,0,0,0,1,1,0,0,1,1],dtype=bool)\n",
    "    #train_mask=torch.ones(len(sentences),dtype=bool)\n",
    "    val_mask=torch.ones(len(sentences),dtype=bool)\n",
    "    test_mask=torch.ones(len(sentences),dtype=bool)\n",
    "\n",
    "    data=Data(train_mask=train_mask,val_mask=val_mask,test_mask=test_mask,y=labels)\n",
    "\n",
    "    # data.child_parent={0:[1,3],1:[-1],2:[-1],3:[1,2]}\n",
    "    # data.parent_child={-1:[1,2],1:[0,3],2:[3],3:[0]}\n",
    "    # data.depth={0:[1,2],1:[0],2:[0],3:[1,1]}\n",
    "\n",
    "    data.child_parent={0:[1],1:[-1],2:[-1],3:[-1]}\n",
    "    data.parent_child={-1:[1,2,3],1:[0]}\n",
    "    data.depth={0:1,1:0,2:0,3:0}\n",
    "\n",
    "    sentences.extend(class_sentences)\n",
    "    data.y=torch.cat((data.y,class_labels),dim=0).type(torch.long)\n",
    "    labels=data.y\n",
    "\n",
    "    class_mask=torch.cat((torch.zeros(len(data.train_mask),dtype=bool),torch.ones(len(class_labels),dtype=bool)),dim=0)\n",
    "    data.class_mask=class_mask\n",
    "\n",
    "    data.train_mask=torch.cat((data.train_mask,torch.zeros(len(class_labels),dtype=bool)),dim=0)\n",
    "    data.val_mask=torch.cat((data.val_mask,torch.zeros(len(class_labels),dtype=bool)),dim=0)\n",
    "    data.test_mask=torch.cat((data.test_mask,torch.zeros(len(class_labels),dtype=bool)),dim=0)\n",
    "\n",
    "    labels=labels.float()\n",
    "    #labels=torch.argmax(data.y,dim=1)\n",
    "    \n",
    "    return data, sentences, labels\n",
    "\n",
    "#data, sentences, labels=getDummyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human machine interface for lab abc computer applications',\n",
       " 'A survey of user opinion of computer system response time',\n",
       " 'The EPS user interface management system',\n",
       " 'System anad human system engineering testing of EPS',\n",
       " 'Relation of user perceived response time to error measurement',\n",
       " 'The generation of random binary unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV Widths of trees and well quasi ordering',\n",
       " 'Graph minors A survey',\n",
       " 'The quick brown fox jumps over the lazy dog',\n",
       " \"it's been raining cats and dogs\",\n",
       " \"I don't know what to think any more\",\n",
       " 'I love animals',\n",
       " 'Human machine interaction of computer',\n",
       " 'Graph tress algorithm testing',\n",
       " 'EPS user are management',\n",
       " 'Animal dogs are animals too']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
