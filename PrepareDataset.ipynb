{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************\n",
    "\n",
    "        V2W-BERT: A Python library for vulnerability classification\n",
    "               Siddhartha Das (das90@purdue.edu) : Purdue University\n",
    "               Mahantesh Halappanavar (hala@pnnl.gov): Pacific Northwest National Laboratory   \n",
    "\n",
    "***********************************************************************\n",
    "\n",
    " \n",
    "Copyright Â© 2022, Battelle Memorial Institute\n",
    "All rights reserved.\n",
    "\n",
    " \n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    " \n",
    "1. Redistributions of source code must retain the above copyright notice, this\n",
    "\n",
    "   list of conditions and the following disclaimer.\n",
    "\n",
    " \n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "\n",
    "   this list of conditions and the following disclaimer in the documentation\n",
    "\n",
    "   and/or other materials provided with the distribution.\n",
    "\n",
    " \n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Preprocess Latest Dataset\n",
    "\n",
    "In this script we first download all CVEs to-date. Use the NVD and Mitre hierarchy documents to prepare a train test validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests, zipfile, io\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Here, I have disabled a false alarm that would otherwise trip later in the project.\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# The datetime library will let me filter the data by reporting date.\n",
    "from datetime import datetime, timedelta\n",
    "# Since the NVD data is housed in JavaScript Object Notation (JSON) format, I will need the json_normalize function to access and manipulate the information.\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from ipynb.fs.full.Dataset import  Data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expanding view area to facilitate data manipulation.\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dir': 'Dataset', 'from_year': 2020, 'to_year': 2022, 'from_train_year': 1990, 'to_train_year': 2020, 'from_test_year': 2021, 'to_test_year': 2021, 'from_val_year': 2022, 'to_val_year': 2022, 'f': '/home/das90/.local/share/jupyter/runtime/kernel-75d01cc4-ad3e-4567-bc03-c1d5634ac6c8.json'}\n",
      "Dataset\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--dir', type=str, default='Dataset')\n",
    "    \n",
    "    parser.add_argument('--from_year', type=int, default=2020)\n",
    "    parser.add_argument('--to_year', type=int, default=2022)\n",
    "    \n",
    "    parser.add_argument('--from_train_year', type=int, default=1990)\n",
    "    parser.add_argument('--to_train_year', type=int, default=2020)\n",
    "    \n",
    "    parser.add_argument('--from_test_year', type=int, default=2021)\n",
    "    parser.add_argument('--to_test_year', type=int, default=2021)\n",
    "    \n",
    "    parser.add_argument('--from_val_year', type=int, default=2022)\n",
    "    parser.add_argument('--to_val_year', type=int, default=2022)\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    args = parser.parse_args()    \n",
    "    dict_args = vars(args)\n",
    "\n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args=get_configuration()\n",
    "\n",
    "print(dict_args)\n",
    "print(args.dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DataPath():\n",
    "    def __init__(self, args, dataset_dir='',results_dir=''):\n",
    "        \n",
    "        #File locations\n",
    "        self.PATH_TO_DATASETS_DIRECTORY = dataset_dir+'/NVD/raw/'\n",
    "        self.PATH_TO_RESULTS_DIRECTORY = results_dir+'/NVD/processed/'\n",
    "        \n",
    "        self.NVD_CVE_FILE=self.PATH_TO_RESULTS_DIRECTORY+'NVD_CVE_data.csv'\n",
    "        self.Graph_FILE=self.PATH_TO_RESULTS_DIRECTORY+'GRAPH_data'\n",
    "        self.GRAPHVIZ_HIERARCHY=self.PATH_TO_RESULTS_DIRECTORY+'Hierarchy'\n",
    "        \n",
    "        self.MITRE_CWE_FILE=self.PATH_TO_DATASETS_DIRECTORY+'CWE_RC_1000.csv'\n",
    "        self.NVD_CWE_FILE=self.PATH_TO_RESULTS_DIRECTORY+'NVD_CWE_data.csv'        \n",
    "        \n",
    "        self.MASK_FILE = self.PATH_TO_RESULTS_DIRECTORY+'NVD_data'\n",
    "        self.MERGED_NVD_CVE_FILE=self.PATH_TO_RESULTS_DIRECTORY+'NVD_CVE.csv'\n",
    "        self.FILTERED_NVD_CWE_FILE=self.PATH_TO_RESULTS_DIRECTORY+'NVD_CWE.csv'\n",
    "        \n",
    "        self.YEARS=list(range(args.from_year,args.to_year+1))\n",
    "        \n",
    "        self.TRAIN_YEARS=list(range(args.from_train_year,args.to_train_year+1))\n",
    "        self.VAL_YEARS=list(range(args.from_val_year,args.to_val_year+1))\n",
    "        self.TEST_YEARS=list(range(args.from_test_year,args.to_test_year+1)) \n",
    "        \n",
    "        \n",
    "        if not os.path.exists(self.PATH_TO_DATASETS_DIRECTORY):\n",
    "            print(\"Creating directory: \",self.PATH_TO_DATASETS_DIRECTORY)\n",
    "            os.makedirs(self.PATH_TO_DATASETS_DIRECTORY)\n",
    "        if not os.path.exists(self.PATH_TO_RESULTS_DIRECTORY):\n",
    "            print(\"Creating directory: \",self.PATH_TO_RESULTS_DIRECTORY)\n",
    "            os.makedirs(self.PATH_TO_RESULTS_DIRECTORY)\n",
    "\n",
    "\n",
    "class Config(DataPath):\n",
    "    def __init__(self,args, dataset_dir='',results_dir=''):\n",
    "        super(Config, self).__init__(args, dataset_dir, results_dir)\n",
    "        self.CLUSTER_LABEL=0\n",
    "    \n",
    "        self.download()\n",
    "        \n",
    "    def download(self):        \n",
    "        for year in self.YEARS:            \n",
    "            if not os.path.exists(self.PATH_TO_DATASETS_DIRECTORY+'nvdcve-1.1-'+str(year)+'.json'):\n",
    "                url = 'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-'+str(year)+'.json.zip'\n",
    "                print(\"Downloading: \",url)\n",
    "                r = requests.get(url)\n",
    "                z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "                z.extractall(self.PATH_TO_DATASETS_DIRECTORY)\n",
    "                print(\"CVEs downloaded\")\n",
    "         \n",
    "        if not os.path.exists(self.MITRE_CWE_FILE):\n",
    "            url = 'https://drive.google.com/uc?export=download&id=1-phSamb4RbxyoBc3AQ2xxKMSsK2DwPyn'\n",
    "            print(\"Downloading: \",url)\n",
    "            r = requests.get(url)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(self.PATH_TO_DATASETS_DIRECTORY)\n",
    "            print(\"CWEs downloaded\")\n",
    "            \n",
    "\n",
    "config=Config(args,dataset_dir=args.dir,results_dir=args.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProfecessCVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getDataFrame(config):\n",
    "    df = []\n",
    "    counter=0\n",
    "    for year in config.YEARS:    \n",
    "        yearly_data = pd.read_json(config.PATH_TO_DATASETS_DIRECTORY+'nvdcve-1.1-'+str(year)+'.json')        \n",
    "        \n",
    "        if counter == 0:\n",
    "            df = yearly_data\n",
    "        else:\n",
    "            df = df.append(yearly_data)\n",
    "        counter+=1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def removeREJECT(description):\n",
    "    series=[]\n",
    "    for x in description:\n",
    "        try:        \n",
    "            if \"REJECT\" in (json_normalize(x)[\"value\"])[0]:                    \n",
    "                series.append(False)\n",
    "            else:\n",
    "                series.append(True)            \n",
    "        except:\n",
    "            series.append(False)\n",
    "    \n",
    "    return pd.Series(series,index=description.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def removeUnknownCWE(description):    \n",
    "    series=[]\n",
    "    for x in description:\n",
    "        try:        \n",
    "            if x == \"UNKNOWN\" or x == \"NONE\":\n",
    "                series.append(False)\n",
    "            else:\n",
    "                series.append(True)            \n",
    "        except:\n",
    "            series.append(False)\n",
    "    \n",
    "    return pd.Series(series,index=description.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getCVEDescription(df):\n",
    "    CVE_entry = []\n",
    "    CVE_index = df[\"cve.description.description_data\"].index\n",
    "\n",
    "    for x in df[\"cve.description.description_data\"]:\n",
    "        try:\n",
    "            raw_CVE_entry = json_normalize(x)[\"value\"][0]\n",
    "            clean_CVE_entry = str(raw_CVE_entry)\n",
    "            CVE_entry.append(clean_CVE_entry)\n",
    "        except:\n",
    "            CVE_entry.append(\"NONE\")\n",
    "    CVE_entry = pd.Series(CVE_entry, index = CVE_index)\n",
    "    \n",
    "    return  CVE_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Defining a function which I will use below\n",
    "def consolidate_unknowns(x):\n",
    "    if x == \"NVD-CWE-Other\" or x == \"NVD-CWE-noinfo\":\n",
    "        return \"UNKNOWN\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getCWEs(df):\n",
    "    CWE_entry = []\n",
    "    CWE_index = df[\"cve.problemtype.problemtype_data\"].index\n",
    "\n",
    "    for x in df[\"cve.problemtype.problemtype_data\"]:\n",
    "        try:\n",
    "            CWE_normalized_json_step_1 = json_normalize(x)\n",
    "            CWE_normalized_json_step_2 = CWE_normalized_json_step_1[\"description\"][0]\n",
    "            CWEs=[]\n",
    "            #print(json_normalize(CWE_normalized_json_step_2)[\"value\"])\n",
    "            for CWE in json_normalize(CWE_normalized_json_step_2)[\"value\"]:\n",
    "                #CWEs.append(consolidate_unknowns(str(CWE)))\n",
    "                CWEs.append(str(CWE))\n",
    "            CWE_entry.append(CWEs)\n",
    "        except:\n",
    "            CWE_entry.append(['NONE'])\n",
    "    CWE_entry = pd.Series(CWE_entry, index = CWE_index)\n",
    "    \n",
    "    return  CWE_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ProcessDataset(config):\n",
    "    print(\"Loading data from file---\")\n",
    "    df=getDataFrame(config)\n",
    "    CVE_Items = json_normalize(df[\"CVE_Items\"])\n",
    "    df = pd.concat([df.reset_index(), CVE_Items], axis=1)\n",
    "    df = df.drop([\"index\", \"CVE_Items\"], axis=1)\n",
    "    \n",
    "    df = df.rename(columns={\"cve.CVE_data_meta.ID\": \"CVE ID\"})\n",
    "    CVE_ID = df[\"CVE ID\"]\n",
    "    df.drop(labels=[\"CVE ID\"], axis=1,inplace = True)\n",
    "    df.insert(0, \"CVE ID\", CVE_ID)\n",
    "    \n",
    "    ##remove description with REJECT\n",
    "    print(\"Removing REJECTs---\")\n",
    "    df=df[removeREJECT(df[\"cve.description.description_data\"])]\n",
    "    \n",
    "    ##Extract CVE description\n",
    "    CVE_description=getCVEDescription(df)\n",
    "    df.insert(1, \"CVE Description\", CVE_description)\n",
    "        \n",
    "    ##Extract CWEs\n",
    "    print(\"Extracting CWEs---\")\n",
    "    CWE_entry=getCWEs(df)\n",
    "    df.insert(2, \"CWE Code\", CWE_entry)\n",
    "    \n",
    "#     ##Remove CWEs we don't know true label\n",
    "#     print(\"Removing Unknown CWEs---\")\n",
    "#     df=df[removeUnknownCWE(df[\"CWE Code 1\"])]\n",
    "    \n",
    "    # Converting the data to pandas date-time format\n",
    "    df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcessCWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def processAndSaveCVE(config, LOAD_SAVED=True):\n",
    "    \n",
    "    if not os.path.exists(config.NVD_CVE_FILE) or LOAD_SAVED==False:\n",
    "        df=ProcessDataset(config)\n",
    "        df=df[['publishedDate', 'CVE ID', 'CVE Description', 'CWE Code']]                \n",
    "        df.to_csv(config.NVD_CVE_FILE,index=False)\n",
    "    else:\n",
    "        df=pd.read_csv(config.NVD_CVE_FILE)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ProcessCWE_NVD(config):\n",
    "    # Importing BeautifulSoup and an xml parser to scrape the CWE definitions from the NVD web site\n",
    "    from bs4 import BeautifulSoup\n",
    "    import lxml.etree\n",
    "    \n",
    "    # loading the NVD CWE Definitions page and scraping it for the first table that appears\n",
    "    NVD_CWE_description_url = requests.get(\"https://nvd.nist.gov/vuln/categories\")\n",
    "    CWE_definitions_page_soup =  BeautifulSoup(NVD_CWE_description_url.content, \"html.parser\")\n",
    "    table = CWE_definitions_page_soup.find_all('table')[0] \n",
    "    df_CWE_definitions = pd.read_html(str(table))[0]\n",
    "    \n",
    "    return df_CWE_definitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ProcessCWE_MITRE(config):\n",
    "    print('Loading CWE file : {0}'.format(config.MITRE_CWE_FILE))\n",
    "    #df_CWE_definitions = pd.read_csv(config.MITRE_CWE_FILE, quotechar='\"',delimiter=',', encoding='latin1',index_col=False)    \n",
    "    df_CWE_definitions = pd.read_csv(config.MITRE_CWE_FILE, delimiter=',', encoding='latin1',index_col=False)\n",
    "    \n",
    "    return df_CWE_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processAndSaveCWE(config, LOAD_SAVED=True):\n",
    "    \n",
    "    if not os.path.exists(config.MITRE_CWE_FILE) or LOAD_SAVED==False:        \n",
    "        df_CWE_MITRE=ProcessCWE_MITRE(config)\n",
    "        df_CWE_MITRE.to_csv(config.MITRE_CWE_FILE,index=False)\n",
    "    else:\n",
    "        df_CWE_MITRE=pd.read_csv(config.MITRE_CWE_FILE, index_col=False)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(config.NVD_CWE_FILE) or LOAD_SAVED==False:        \n",
    "        df_CWE_NVD=ProcessCWE_NVD(config)\n",
    "        df_CWE_NVD.to_csv(config.NVD_CWE_FILE,index=False)\n",
    "    else:\n",
    "        df_CWE_NVD=pd.read_csv(config.NVD_CWE_FILE,index_col=False)\n",
    "    \n",
    "    \n",
    "    return df_CWE_MITRE, df_CWE_NVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_CWE_MITRE, df_CWE_NVD = processAndSaveCWE(config, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_CWE_MITRE\n",
    "#df_CWE_NVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_preprocessed(config, LOAD_SAVED=True):\n",
    "    \n",
    "    df_CVE=processAndSaveCVE(config, LOAD_SAVED)\n",
    "    df_CWE_MITRE, df_CWE_NVD = processAndSaveCWE(config, LOAD_SAVED=True)\n",
    "    \n",
    "    index1= np.argwhere(df_CWE_NVD['Name'].values == 'NVD-CWE-Other')[0][0]\n",
    "    index2= np.argwhere(df_CWE_NVD['Name'].values == 'NVD-CWE-noinfo')[0][0]\n",
    "    \n",
    "    df_CWE_NVD.drop(index=[index1,index2], inplace = True)\n",
    "    \n",
    "    return df_CVE, df_CWE_NVD, df_CWE_MITRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_preprocessed(config, LOAD_SAVED=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getMask(config,df_CVE,df_CWE):\n",
    "    \n",
    "    n = len(df_CWE)\n",
    "    m = len(df_CVE)\n",
    "    \n",
    "    #get date range\n",
    "    train_start_date = pd.to_datetime(str(config.TRAIN_YEARS[0])+'-01-01').tz_localize('US/Eastern')\n",
    "    train_end_date = pd.to_datetime(str(config.TRAIN_YEARS[-1])+'-01-01').tz_localize('US/Eastern') + timedelta(days=365)\n",
    "    val_start_date = pd.to_datetime(str(config.VAL_YEARS[0])+'-01-01').tz_localize('US/Eastern')\n",
    "    val_end_date = pd.to_datetime(str(config.VAL_YEARS[-1])+'-01-01').tz_localize('US/Eastern') + timedelta(days=365)\n",
    "    test_start_date = pd.to_datetime(str(config.TEST_YEARS[0])+'-01-01').tz_localize('US/Eastern')\n",
    "    test_end_date = pd.to_datetime(str(config.TEST_YEARS[-1])+'-01-01').tz_localize('US/Eastern') + timedelta(days=365)\n",
    "    \n",
    "    cwe_ids=df_CWE['Name']    \n",
    "    cwe_map=dict(zip(cwe_ids, list(range(n))))\n",
    "    index_cwe_map = dict(zip(list(range(n)),cwe_ids))\n",
    "\n",
    "    #creating y and finding labeled \n",
    "    y=torch.zeros((m,n),dtype=torch.long)    \n",
    "    labeled_mask= torch.zeros(m, dtype=torch.bool)\n",
    "    train_index = torch.zeros(m, dtype=torch.bool)\n",
    "    test_index = torch.zeros(m, dtype=torch.bool)\n",
    "    val_index = torch.zeros(m, dtype=torch.bool)\n",
    "    \n",
    "    CWEs=df_CVE['CWE Code']\n",
    "    Dates=df_CVE['publishedDate']\n",
    "    \n",
    "    for i,row in enumerate(zip(CWEs,Dates)):        \n",
    "        cwes=row[0]\n",
    "        date=row[1]\n",
    "        \n",
    "        if(type(cwes) == str):\n",
    "            cwes=[cwe for cwe in cwes.strip('[]').split(\"'\") if not (cwe==',' or cwe==', ' or cwe=='''''')]        \n",
    "        \n",
    "        if(type(date) == str):\n",
    "            date=pd.to_datetime(date)            \n",
    "        \n",
    "        for cwe in cwes:\n",
    "            if cwe in cwe_map:                \n",
    "                y[i][cwe_map[cwe]]=1\n",
    "            \n",
    "        if torch.sum(y[i])>0:\n",
    "            labeled_mask[i]=True            \n",
    "            \n",
    "            if(train_start_date<date and date<train_end_date):\n",
    "                train_index[i]=True                \n",
    "            elif(val_start_date<date and date<val_end_date):\n",
    "                val_index[i]=True\n",
    "            elif(test_start_date<date and date<test_end_date):\n",
    "                test_index[i]=True\n",
    "            else:                \n",
    "                print(date,'-> not covered')\n",
    "    \n",
    "    ##convert to tensors\n",
    "    data=Data(train_mask=train_index, val_mask=val_index, test_mask=test_index, y=y, num_nodes=m)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def getPercent(data,df_CVE,df_CWE, max_data_inaclass=500):\n",
    "    CWEs=df_CVE['CWE Code']\n",
    "    \n",
    "    train_mask= (data.train_mask == True).nonzero().flatten().numpy()\n",
    "    \n",
    "    CWEs_train={}\n",
    "    for key in train_mask:\n",
    "        cwes=CWEs[key]\n",
    "        if(type(cwes) == str):\n",
    "            cwes=[cwe.strip() for cwe in cwes.strip('[]').split(\"'\") if not (cwe==',' or cwe==', ' or cwe=='''''')]        \n",
    "        \n",
    "        for cwe in cwes:           \n",
    "            if cwe in CWEs_train: \n",
    "                CWEs_train[cwe].append(key)\n",
    "            else: \n",
    "                CWEs_train[cwe]=[key]\n",
    "            \n",
    "    \n",
    "    required_train_mask = torch.zeros(len(data.train_mask), dtype=torch.bool)\n",
    "        \n",
    "    for key, values in CWEs_train.items(): \n",
    "        if(len(values)<max_data_inaclass):\n",
    "            required_train_mask[values]=True\n",
    "        else:\n",
    "            np.random.shuffle(values)\n",
    "            takeamnt=max_data_inaclass\n",
    "            required_train_mask[values[:takeamnt]]=True\n",
    "                \n",
    "    data.train_mask=required_train_mask    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "code_folding": [
     2,
     5,
     12
    ]
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def CWE_description(row):    \n",
    "    return str(row['Name'])+\" \"+str(row['Description'])+\" \"+str(row['Extended Description'])+\" \"+str(row['Common Consequences'])\n",
    "\n",
    "def CWE_description_NVD(row,df_CWE_Mitre):\n",
    "    cwe=row['Name']\n",
    "    cwe_id = int(re.findall(\"\\d+\", cwe)[0])\n",
    "    \n",
    "    description = df_CWE_Mitre[df_CWE_Mitre['CWE-ID'].values==cwe_id]['CVE Description'].values\n",
    "    if len(description)>0:\n",
    "        return description[0]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def UpdateData(data, df_CVE, df_CWE_NVD, df_CWE_MITRE):\n",
    "    \n",
    "    df_CWE_MITRE['CVE Description']= df_CWE_MITRE.apply(lambda row: CWE_description(row), axis=1)   \n",
    "    \n",
    "    for i, row in df_CWE_NVD.iterrows():\n",
    "        description=CWE_description_NVD(row, df_CWE_MITRE)\n",
    "        #df_CWE_NVD.set_value(i,'CVE Description',description)\n",
    "        df_CWE_NVD.at[i,'CVE Description']=description\n",
    "\n",
    "    df_CWE_NVD['CWE Code']= df_CWE_NVD.apply(lambda row: [str(row['Name'])], axis=1)\n",
    "    df_CWE_NVD=df_CWE_NVD[['CVE Description','CWE Code','Name']]\n",
    "    \n",
    "    df_CVE_updated = pd.concat([df_CVE,df_CWE_NVD],ignore_index=True, sort=False)\n",
    "\n",
    "    n = len(df_CWE_NVD)  \n",
    "    cwe_ids=df_CWE_NVD['Name']    \n",
    "    cwe_map=dict(zip(cwe_ids, list(range(n))))\n",
    "    index_cwe_map = dict(zip(list(range(n)),cwe_ids))\n",
    "\n",
    "    class_labels=torch.zeros((n,n),dtype=torch.long)\n",
    "    \n",
    "    CWElist=df_CWE_NVD['Name'].tolist()\n",
    "    for i,cwe in enumerate(CWElist):\n",
    "        cwe_value=cwe\n",
    "        class_labels[i][cwe_map[cwe_value]]=1\n",
    "    \n",
    "    data.y=torch.cat((data.y,class_labels),dim=0)\n",
    "\n",
    "    class_mask=torch.cat((torch.zeros(len(data.train_mask),dtype=bool),torch.ones(len(class_labels),dtype=bool)),dim=0)\n",
    "    data.class_mask=class_mask\n",
    "\n",
    "    data.train_mask=torch.cat((data.train_mask,torch.zeros(len(class_labels),dtype=bool)),dim=0)\n",
    "    data.val_mask=torch.cat((data.val_mask,torch.zeros(len(class_labels),dtype=bool)),dim=0)\n",
    "    data.test_mask=torch.cat((data.test_mask,torch.zeros(len(class_labels),dtype=bool)),dim=0)\n",
    "\n",
    "    return data, df_CVE_updated, df_CWE_NVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWE hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "code_folding": [
     0,
     18,
     31,
     41,
     69
    ]
   },
   "outputs": [],
   "source": [
    "def cwe_child(row):\n",
    "    item = str(row['Related Weaknesses'])\n",
    "    cve_p = re.compile('ChildOf:CWE ID:\\\\d+')\n",
    "    results = cve_p.findall(item)\n",
    "    item=''.join(results)\n",
    "    cve_p = re.compile('\\\\d+')\n",
    "    results = cve_p.findall(item)\n",
    "    results = list(map(int, results))\n",
    "    results=list(OrderedDict.fromkeys(results)) #preserve order\n",
    "    #results=list(set(results)) #order not preserve\n",
    "\n",
    "    # print(str(row['CWE-ID'])+'->', end=\"\")\n",
    "    # print(results)\n",
    "    if(len(results)>0):\n",
    "        return results\n",
    "    else:\n",
    "        return [-1]\n",
    "\n",
    "def depth_node(child_parent,node):\n",
    "    \n",
    "    if -1 in child_parent[node]:\n",
    "        return [0]\n",
    "    \n",
    "    depths=[]\n",
    "    \n",
    "    for parent_node in child_parent[node]:\n",
    "        parent_depth=depth_node(child_parent, parent_node)        \n",
    "        depths.extend([x+1 for x in parent_depth])\n",
    "        \n",
    "    return depths\n",
    "                        \n",
    "def create_group(nodes,parents):\n",
    "    \n",
    "    child_parent=dict(zip(nodes,parents))\n",
    "    depth={}\n",
    "\n",
    "    for node, level in child_parent.items():\n",
    "        depth[node]=depth_node(child_parent, node)\n",
    "    \n",
    "    return child_parent, depth\n",
    "\n",
    "def save_hierarchy(config, nodes, names, child_parent):\n",
    "    from graphviz import Digraph,Graph\n",
    "    #save hierarchy graph\n",
    "    dot = Digraph(comment='NVD Research Concepts Hierarchy',engine='dot',node_attr={'shape': 'box'})\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    #dot=Graph(format='png')\n",
    "    \n",
    "    root=1003\n",
    "    dot.node(str(root), \"CWE-ID \" + str(root) + \":\" + 'Root Node')\n",
    "    \n",
    "    for i in range(len(nodes)):\n",
    "        dot.node(str(nodes[i]), \"CWE-ID \" + str(nodes[i]) + \":\" + names[i])\n",
    "\n",
    "\n",
    "    for cwe in nodes:\n",
    "        parents=child_parent[cwe]\n",
    "        \n",
    "        if(parents[0]==-1):\n",
    "            dot.edge(str(cwe),str(root))\n",
    "            continue\n",
    "            \n",
    "        for p in parents:\n",
    "            dot.edge(str(cwe),str(p))\n",
    "\n",
    "    #print(dot.source)\n",
    "    dot.format='pdf'\n",
    "    dot.render(config.GRAPHVIZ_HIERARCHY, view=False)\n",
    "\n",
    "def cluster_cwes(config, cwe_c,df_CWE_NVD):\n",
    "    \n",
    "    valid_cwes=[]\n",
    "    for cwe in df_CWE_NVD['Name']:\n",
    "        if cwe in ['NVD-CWE-Other','NVD-CWE-noinfo']: continue    \n",
    "        cwe_id = int(re.findall(\"\\d+\", cwe)[0])\n",
    "        valid_cwes.append(cwe_id)\n",
    "    \n",
    "    delete_indexs=[]\n",
    "    for i, row in cwe_c.iterrows():\n",
    "        cwe=int(row['CWE-ID'])\n",
    "        if(cwe not in valid_cwes):\n",
    "            delete_indexs.append(i)\n",
    "    \n",
    "    cwe_c.drop(delete_indexs,inplace=True)\n",
    "    \n",
    "    parent_columns=[]\n",
    "    for i, row in cwe_c.iterrows():\n",
    "        parents=cwe_child(row)\n",
    "        valid_parents=[]\n",
    "\n",
    "        for x in parents:\n",
    "            if x in valid_cwes:\n",
    "                valid_parents.append(x)\n",
    "        \n",
    "        if(len(valid_parents)==0):\n",
    "            valid_parents.append(-1)\n",
    "        \n",
    "        parent_columns.append(valid_parents)\n",
    "    \n",
    "    cwe_c['parent']=parent_columns\n",
    "    \n",
    "    nodes=cwe_c['CWE-ID'].tolist()\n",
    "    parents=cwe_c['parent'].tolist()\n",
    "    \n",
    "    child_parent, depth = create_group(nodes, parents)\n",
    "    \n",
    "    save_hierarchy(config, nodes, cwe_c['Name'].tolist(),child_parent)\n",
    "\n",
    "    return child_parent, depth\n",
    "\n",
    "# df_CWE_MITRE, df_CWE_NVD = processAndSaveCWE(config, LOAD_SAVED=True)\n",
    "# child_parent, depth = cluster_cwes(config, df_CWE_MITRE,df_CWE_NVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_heirarchy(data,df_CWE_NVD):\n",
    "    from ipynb.fs.full.NVD_cwe_hierarchy import get_nvd_hierarchy\n",
    "    org_child_parent, org_parent_child, org_depth = get_nvd_hierarchy()\n",
    "    data.org_child_parent=org_child_parent\n",
    "    data.org_parent_child=org_parent_child\n",
    "    data.org_depth=org_depth\n",
    "\n",
    "    n = len(df_CWE_NVD)    \n",
    "    cwe_ids=df_CWE_NVD['Name'].tolist()\n",
    "    \n",
    "    if 'NVD-CWE-noinfo' in cwe_ids: cwe_ids.remove('NVD-CWE-noinfo')\n",
    "    if 'NVD-CWE-Other' in cwe_ids: cwe_ids.remove('NVD-CWE-Other')\n",
    "    \n",
    "    cwe_ids=[int(re.findall(\"\\d+\", cwe)[0]) for cwe in cwe_ids]\n",
    "    \n",
    "    cwe_map=dict(zip(cwe_ids, list(range(n))))\n",
    "    index_cwe_map = dict(zip(list(range(n)),cwe_ids))\n",
    "    \n",
    "    child_parent={}\n",
    "    for c,p in org_child_parent.items():\n",
    "        if -1 in p:\n",
    "            child_parent[cwe_map[c]]=[-1 for px in p]\n",
    "        else:    \n",
    "            child_parent[cwe_map[c]]=[cwe_map[px] for px in p]\n",
    "    \n",
    "    parent_child={}\n",
    "    for p,c in org_parent_child.items():\n",
    "        if p==-1:\n",
    "            parent_child[-1]=[cwe_map[cx] for cx in c]\n",
    "        else:            \n",
    "            parent_child[cwe_map[p]]=[cwe_map[cx] for cx in c]\n",
    "        \n",
    "    depth={}\n",
    "    for i,d in org_depth.items():\n",
    "        depth[cwe_map[i]]=d\n",
    "        \n",
    "    data.child_parent=child_parent\n",
    "    data.parent_child=parent_child\n",
    "    data.depth=depth\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# df_CWE_MITRE, df_CWE_NVD = processAndSaveCWE(config, LOAD_SAVED=True)\n",
    "# set_heirarchy(Data(),df_CWE_NVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mitre_heirarchy(config, data, df_CWE_MITRE, df_CWE_NVD):\n",
    "    \n",
    "    org_child_parent, org_depth = cluster_cwes(config, df_CWE_MITRE,df_CWE_NVD)\n",
    "    \n",
    "    org_parent_child={}\n",
    "    \n",
    "    for c,p in org_child_parent.items():\n",
    "        for px in p:                \n",
    "            if px in org_parent_child:\n",
    "                org_parent_child[px].append(c)\n",
    "            else:\n",
    "                org_parent_child[px]=[c]\n",
    "    \n",
    "    data.org_child_parent=org_child_parent\n",
    "    data.org_parent_child=org_parent_child\n",
    "    data.org_depth=org_depth\n",
    "\n",
    "    n = len(df_CWE_NVD)    \n",
    "    cwe_ids=df_CWE_NVD['Name'].tolist()\n",
    "    \n",
    "    if 'NVD-CWE-noinfo' in cwe_ids: cwe_ids.remove('NVD-CWE-noinfo')\n",
    "    if 'NVD-CWE-Other' in cwe_ids: cwe_ids.remove('NVD-CWE-Other')\n",
    "    \n",
    "    cwe_ids=[int(re.findall(\"\\d+\", cwe)[0]) for cwe in cwe_ids]\n",
    "    \n",
    "    cwe_map=dict(zip(cwe_ids, list(range(n))))\n",
    "    index_cwe_map = dict(zip(list(range(n)),cwe_ids))\n",
    "    \n",
    "    child_parent={}\n",
    "    for c,p in org_child_parent.items():\n",
    "        if -1 in p:\n",
    "            child_parent[cwe_map[c]]=[-1 for px in p]\n",
    "        else:    \n",
    "            child_parent[cwe_map[c]]=[cwe_map[px] for px in p]\n",
    "    \n",
    "    parent_child={}\n",
    "    for p,c in org_parent_child.items():\n",
    "        if p==-1:\n",
    "            parent_child[-1]=[cwe_map[cx] for cx in c]\n",
    "        else:            \n",
    "            parent_child[cwe_map[p]]=[cwe_map[cx] for cx in c]\n",
    "        \n",
    "    depth={}\n",
    "    for i,d in org_depth.items():\n",
    "        depth[cwe_map[i]]=d\n",
    "        \n",
    "    data.child_parent=child_parent\n",
    "    data.parent_child=parent_child\n",
    "    data.depth=depth\n",
    "    \n",
    "    return data\n",
    "\n",
    "# if os.uname()[0].find('Darwin')==-1: ##if not darwin(mac/locallaptop)\n",
    "#     DIR='/scratch/gilbreth/das90/Dataset'\n",
    "# else:\n",
    "#     DIR='/Users/siddharthashankardas/Purdue/Dataset'  \n",
    "\n",
    "# config=Config(dataset_dir=DIR,results_dir=DIR)\n",
    "# df_CWE_MITRE, df_CWE_NVD = processAndSaveCWE(config, LOAD_SAVED=True)\n",
    "# set_mitre_heirarchy(config, Data(),df_CWE_MITRE, df_CWE_NVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_only(config,df_CVE,df_CWE):\n",
    "    \n",
    "    n = len(df_CWE)\n",
    "    m = len(df_CVE)\n",
    "        \n",
    "    cwe_ids=df_CWE['Name']    \n",
    "    cwe_map=dict(zip(cwe_ids, list(range(n))))\n",
    "    labeled_mask= torch.zeros(m, dtype=torch.bool)\n",
    "    \n",
    "    CWEs=df_CVE['CWE Code']\n",
    "    Dates=df_CVE['publishedDate']\n",
    "    \n",
    "    for i,row in enumerate(zip(CWEs,Dates)):        \n",
    "        cwes=row[0]\n",
    "        date=row[1]\n",
    "        \n",
    "        if(type(cwes) == str):\n",
    "            cwes=[cwe for cwe in cwes.strip('[]').split(\"'\") if not (cwe==',' or cwe==', ' or cwe=='''''')]        \n",
    "                 \n",
    "        \n",
    "        for cwe in cwes:\n",
    "            if cwe in cwe_map:                \n",
    "                labeled_mask[i]=True\n",
    "                break\n",
    "    \n",
    "    print(sum(labeled_mask))\n",
    "    \n",
    "    labeled_indexs= (labeled_mask == True).nonzero().flatten().numpy()\n",
    "    df_CVE=df_CVE.iloc[labeled_indexs,:]\n",
    "    \n",
    "    return df_CVE\n",
    "\n",
    "def take_subset(config,df_CVE,df_CWE_NVD,take=100):\n",
    "    df_CVE=get_labeled_only(config,df_CVE,df_CWE_NVD)\n",
    "    df_CVE=df_CVE.sample(n = take, replace = False)\n",
    "    \n",
    "    return df_CVE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeature(config, max_data_inaclass=-1, RECOMPUTE=False, LOAD_SAVED=True, take=-1, hierarchy=''):\n",
    "    \n",
    "    save_filename=config.MASK_FILE\n",
    "    nvd_filename=config.MERGED_NVD_CVE_FILE\n",
    "    cwe_filename=config.FILTERED_NVD_CWE_FILE\n",
    "    \n",
    "    if RECOMPUTE==True or not os.path.exists(save_filename) or not os.path.exists(nvd_filename) or not os.path.exists(cwe_filename):\n",
    "        \n",
    "        df_CVE, df_CWE_NVD, df_CWE_MITRE =load_preprocessed(config, LOAD_SAVED)\n",
    "        \n",
    "        if take>-1: \n",
    "            print('Selecting a subset')            \n",
    "            df_CVE=take_subset(config,df_CVE,df_CWE_NVD,take)\n",
    "            print('Done...')\n",
    "        \n",
    "        data=getMask(config,df_CVE,df_CWE_NVD)\n",
    "                \n",
    "        if(max_data_inaclass!=-1): \n",
    "            data=getPercent(data,df_CVE,df_CWE_NVD,max_data_inaclass)\n",
    "        \n",
    "        data, df_CVE_merged, df_CWE = UpdateData(data, df_CVE, df_CWE_NVD, df_CWE_MITRE)\n",
    "        \n",
    "        if hierarchy=='nvd':\n",
    "            print('Using nvd hierarchy')\n",
    "            data=set_heirarchy(data,df_CWE_NVD)            \n",
    "        else:\n",
    "            print('using mitre hierarchy')\n",
    "            data=set_mitre_heirarchy(config, data, df_CWE_MITRE, df_CWE_NVD)\n",
    "        \n",
    "        \n",
    "    \n",
    "        pickle.dump(data,open(save_filename, \"wb\" ))\n",
    "        df_CVE_merged.to_csv(nvd_filename,index=False)\n",
    "        df_CWE.to_csv(cwe_filename,index=False)\n",
    "        \n",
    "    else:\n",
    "        data= pickle.load(open(save_filename, \"rb\" ))\n",
    "        df_CVE_merged=pd.read_csv(nvd_filename,low_memory=False)\n",
    "        df_CWE=pd.read_csv(cwe_filename,low_memory=False)\n",
    "        \n",
    "    return data, df_CVE_merged, df_CWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_CVE, df_CWE_NVD, df_CWE_MITRE = load_preprocessed(config, True)\n",
    "# data=getMask(config,df_CVE,df_CWE_NVD)\n",
    "# data, df_CVE_merged, df_CWE = UpdateData(data, df_CVE, df_CWE_NVD, df_CWE_MITRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-209-5b9cbcbe734d>:4: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  CVE_Items = json_normalize(df[\"CVE_Items\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing REJECTs---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-204-bbccff5c1fab>:5: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  if \"REJECT\" in (json_normalize(x)[\"value\"])[0]:\n",
      "<ipython-input-206-dd53adf00c3b>:7: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  raw_CVE_entry = json_normalize(x)[\"value\"][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CWEs---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-208-97899b8f440c>:11: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  for CWE in json_normalize(CWE_normalized_json_step_2)[\"value\"]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-31 18:15:00+00:00 -> not covered\n",
      "2020-12-31 18:15:00+00:00 -> not covered\n",
      "2020-12-31 18:15:00+00:00 -> not covered\n",
      "2020-12-31 18:15:00+00:00 -> not covered\n",
      "2020-12-31 18:15:00+00:00 -> not covered\n",
      "2020-12-31 18:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 21:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 08:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 10:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 09:15:00+00:00 -> not covered\n",
      "2020-12-31 20:15:00+00:00 -> not covered\n",
      "2020-12-31 21:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 02:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "2021-01-01 04:15:00+00:00 -> not covered\n",
      "using mitre hierarchy\n"
     ]
    }
   ],
   "source": [
    "# if os.uname()[0].find('Darwin')==-1: ##if not darwin(mac/locallaptop)\n",
    "#     DIR='/scratch/gilbreth/das90/Dataset'\n",
    "# else:\n",
    "#     DIR='/Users/siddharthashankardas/Purdue/Dataset'  \n",
    "\n",
    "#config=Config(dataset_dir=DIR,results_dir=DIR)\n",
    "\n",
    "data, df_CVE, df_CWE=ExtractFeature(config,max_data_inaclass=-1, RECOMPUTE=True, LOAD_SAVED=False, take=-1, hierarchy='mitre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: tensor(10655)\n",
      "Val size: tensor(2719)\n",
      "Test size: tensor(16747)\n",
      "Class size: tensor(127)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", sum(data.train_mask))\n",
    "print(\"Val size:\", sum(data.val_mask))\n",
    "print(\"Test size:\", sum(data.test_mask))\n",
    "print(\"Class size:\",sum(data.class_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37797\n"
     ]
    }
   ],
   "source": [
    "print(len(data.train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_CVE, df_CWE_NVD, df_CWE_MITRE =load_preprocessed(config, LOAD_SAVED=True)\n",
    "# take_subset(config,df_CVE,df_CWE_NVD, take=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     if os.uname()[0].find('Darwin')==-1: ##if not darwin(mac/locallaptop)\n",
    "#         DIR='/scratch/gilbreth/das90/Dataset'\n",
    "#     else:\n",
    "#         DIR='/Users/siddharthashankardas/Purdue/Dataset'  \n",
    "\n",
    "#     config=Config(dataset_dir=DIR,results_dir=DIR)\n",
    "#     data, df_CVE, df_CWE = ExtractFeature(config,max_data_inaclass=-1, RECOMPUTE=True, LOAD_SAVED=True)\n",
    "    \n",
    "#     print(\"Train size:\", sum(data.train_mask))\n",
    "#     print(\"Val size:\", sum(data.val_mask))\n",
    "#     print(\"Test size:\", sum(data.test_mask))\n",
    "#     print(\"Class size:\",sum(data.class_mask))\n",
    "    \n",
    "#     print(\"Total: \",sum(data.train_mask)+sum(data.val_mask)+sum(data.test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
